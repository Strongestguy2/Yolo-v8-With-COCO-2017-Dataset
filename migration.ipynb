{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install fiftyone\n",
        "# 1. Imports & Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import copy\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "# Set Seed\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"âœ… Notebook Updated: Version 3.0 (Fixed Collate & Criterion)\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Configuration\n",
        "# --- USER SETTINGS ---\n",
        "QUICK_TEST = True  # Set to True for a fast smoke test\n",
        "NUM_SAMPLES = 5000 if QUICK_TEST else 20000  # Total images on disk\n",
        "TOTAL_EPOCHS = 3 if QUICK_TEST else 100     # Training goal\n",
        "\n",
        "BASE_DIR = os.path.abspath(\"yolo-lab\")\n",
        "DIRS = {\n",
        "    \"datasets\": os.path.join(BASE_DIR, \"datasets\"),\n",
        "    \"runs\": os.path.join(BASE_DIR, \"runs\"),\n",
        "    \"configs\": os.path.join(BASE_DIR, \"configs\"),\n",
        "}\n",
        "for d in DIRS.values():\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "EXP_NAME = \"yolov8_fixed_dataset\"\n",
        "RUN_NAME = f\"{timestamp}_{EXP_NAME}\"\n",
        "RUN_DIR = os.path.join(DIRS[\"runs\"], RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "# Model & Training Config\n",
        "CFG = {\n",
        "    \"exp_name\": EXP_NAME,\n",
        "    \"run_name\": RUN_NAME,\n",
        "    \"seed\": 42,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch_size\": 8 if QUICK_TEST else 16,\n",
        "    \"num_classes\": 80,\n",
        "    \"time_limit\": 3600, # 1 hour default\n",
        "    \n",
        "    # Model\n",
        "    \"width\": 1.0,\n",
        "    \"depth\": 1.0,\n",
        "    \"reg_max\": 16,\n",
        "    \"head_hidden\": 256,\n",
        "    \"backbone\": \"yolov8_cspdarknet\",\n",
        "    \n",
        "    # Optimizer\n",
        "    \"optimizer\": \"adamw\",\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"cosine_schedule\": True,\n",
        "    \"epochs\": TOTAL_EPOCHS,\n",
        "    \"amp\": True,\n",
        "    \"grad_clip_norm\": 10.0,\n",
        "    \"ema_decay\": 0.9998,\n",
        "    \n",
        "    # Loss\n",
        "    \"tal_alpha\": 1.0,\n",
        "    \"tal_beta\": 6.0,\n",
        "    \"tal_topk\": 10,\n",
        "    \"tal_center_radius\": 2.5,\n",
        "    \"loss_weights\": {\"box\": 7.5, \"cls\": 0.5, \"dfl\": 1.5},\n",
        "    \n",
        "    # Augmentation\n",
        "    \"letterbox_pad\": 114,\n",
        "    \"hflip_p\": 0.5,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.7,\n",
        "    \"hsv_v\": 0.4,\n",
        "    \n",
        "    # Paths (Fixed local dataset)\n",
        "    \"data_root\": os.path.join(DIRS[\"datasets\"], \"coco_fixed\"),\n",
        "    \"train_img_dir\": \"images/train\",\n",
        "    \"train_lbl_dir\": \"labels/train\",\n",
        "    \"val_img_dir\": \"images/val\",\n",
        "    \"val_lbl_dir\": \"labels/val\",\n",
        "}\n",
        "\n",
        "print(\"Run Directory:\", RUN_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Cleanup Utilities ---\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def confirm_action(prompt):\n",
        "    ans = input(f\"{prompt} (type 'yes' to confirm): \")\n",
        "    return ans.lower() == 'yes'\n",
        "\n",
        "def clear_last_checkpoint():\n",
        "    \"\"\"Deletes last.pt in the current RUN_DIR\"\"\"\n",
        "    path = os.path.join(RUN_DIR, \"last.pt\")\n",
        "    if os.path.exists(path):\n",
        "        if confirm_action(f\"Clean checkpoint at {path}?\"):\n",
        "            os.remove(path)\n",
        "            print(f\"ðŸ—‘ï¸ Deleted {path}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ No checkpoint found at {path}\")\n",
        "\n",
        "def wipe_photos():\n",
        "    \"\"\"Deletes the images folder in current_batch\"\"\"\n",
        "    path = os.path.join(CFG[\"data_root\"], \"images\")\n",
        "    if os.path.exists(path):\n",
        "        if confirm_action(f\"WIPE ALL PHOTOS in {path}?\"):\n",
        "            shutil.rmtree(path)\n",
        "            print(f\"ðŸ—‘ï¸ Deleted {path}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ No photos found at {path}\")\n",
        "\n",
        "def wipe_data():\n",
        "    \"\"\"Deletes the labels folder in current_batch\"\"\"\n",
        "    path = os.path.join(CFG[\"data_root\"], \"labels\")\n",
        "    if os.path.exists(path):\n",
        "        if confirm_action(f\"WIPE ALL DATA (labels) in {path}?\"):\n",
        "            shutil.rmtree(path)\n",
        "            print(f\"ðŸ—‘ï¸ Deleted {path}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ No data found at {path}\")\n",
        "\n",
        "def full_wipe():\n",
        "    \"\"\"Wipes the entire yolo-lab project directory (runs and datasets)\"\"\"\n",
        "    if confirm_action(\"DANGER: WIPE EVERYTHING (runs, datasets, configs)?\"):\n",
        "        if os.path.exists(BASE_DIR):\n",
        "            shutil.rmtree(BASE_DIR)\n",
        "            print(f\"ðŸ—‘ï¸ Deleted {BASE_DIR}\")\n",
        "            # Re-create structure for safety\n",
        "            for d in DIRS.values():\n",
        "                os.makedirs(d, exist_ok=True)\n",
        "            os.makedirs(RUN_DIR, exist_ok=True)\n",
        "            print(\"âœ… Structure re-initialized.\")\n",
        "\n",
        "print(\"â›‘ï¸ Cleanup Utils Loaded: clear_last_checkpoint(), wipe_photos(), wipe_data(), full_wipe()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Model Architecture\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n",
        "        super().__init__()\n",
        "        if p is None: p = k // 2\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n",
        "    def forward(self, x): return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
        "        super().__init__()\n",
        "        c_ = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
        "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "    def forward(self, x): return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "class C2f(nn.Module):\n",
        "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
        "        super().__init__()\n",
        "        self.c = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
        "        self.cv2 = Conv((2 + n) * self.c, c2, 1)\n",
        "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3)), e=1.0) for _ in range(n))\n",
        "    def forward(self, x):\n",
        "        y = list(self.cv1(x).chunk(2, 1))\n",
        "        y.extend(m(y[-1]) for m in self.m)\n",
        "        return self.cv2(torch.cat(y, 1))\n",
        "\n",
        "class SPPF(nn.Module):\n",
        "    def __init__(self, c1, c2, k=5):\n",
        "        super().__init__()\n",
        "        c_ = c1 // 2\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
        "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        y1 = self.m(x)\n",
        "        y2 = self.m(y1)\n",
        "        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n",
        "\n",
        "class CSPDarknet(nn.Module):\n",
        "    def __init__(self, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        base_c = [64, 128, 256, 512, 1024]\n",
        "        base_d = [3, 6, 6, 3]\n",
        "        self.c = [int(x * width) for x in base_c]\n",
        "        self.d = [max(round(x * depth), 1) if x > 1 else x for x in base_d]\n",
        "        self.stem = Conv(3, self.c[0], 3, 2)\n",
        "        self.stage1 = nn.Sequential(Conv(self.c[0], self.c[1], 3, 2), C2f(self.c[1], self.c[1], n=self.d[0], shortcut=True))\n",
        "        self.stage2 = nn.Sequential(Conv(self.c[1], self.c[2], 3, 2), C2f(self.c[2], self.c[2], n=self.d[1], shortcut=True))\n",
        "        self.stage3 = nn.Sequential(Conv(self.c[2], self.c[3], 3, 2), C2f(self.c[3], self.c[3], n=self.d[2], shortcut=True))\n",
        "        self.stage4 = nn.Sequential(Conv(self.c[3], self.c[4], 3, 2), C2f(self.c[4], self.c[4], n=self.d[3], shortcut=True), SPPF(self.c[4], self.c[4], k=5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        c3 = self.stage2(x)\n",
        "        c4 = self.stage3(c3)\n",
        "        c5 = self.stage4(c4)\n",
        "        return c3, c4, c5\n",
        "\n",
        "class YOLOv8PAFPN(nn.Module):\n",
        "    def __init__(self, c3, c4, c5, out_ch=256, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce5 = Conv(c5, c4, 1, 1)\n",
        "        self.c2f_p4 = C2f(c4 + c4, c4, n=3, shortcut=False)\n",
        "        self.reduce4 = Conv(c4, c3, 1, 1)\n",
        "        self.c2f_p3 = C2f(c3 + c3, c3, n=3, shortcut=False)\n",
        "        self.down3 = Conv(c3, c3, 3, 2)\n",
        "        self.c2f_n4 = C2f(c3 + c4, c4, n=3, shortcut=False)\n",
        "        self.down4 = Conv(c4, c4, 3, 2)\n",
        "        self.c2f_n5 = C2f(c4 + c5, c5, n=3, shortcut=False)\n",
        "\n",
        "    def forward(self, c3, c4, c5):\n",
        "        p5 = c5\n",
        "        p4 = self.reduce5(p5)\n",
        "        p4_out = self.c2f_p4(torch.cat([self.up(p4), c4], dim=1))\n",
        "        p3 = self.reduce4(p4_out)\n",
        "        p3_out = self.c2f_p3(torch.cat([self.up(p3), c3], dim=1))\n",
        "        n3 = p3_out\n",
        "        n4_out = self.c2f_n4(torch.cat([self.down3(n3), p4_out], dim=1))\n",
        "        n5_out = self.c2f_n5(torch.cat([self.down4(n4_out), p5], dim=1))\n",
        "        return n3, n4_out, n5_out\n",
        "\n",
        "class Integral(nn.Module):\n",
        "    def __init__(self, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.reg_max = int(reg_max)\n",
        "        self.register_buffer(\"proj\", torch.arange(self.reg_max + 1, dtype=torch.float32), persistent=False)\n",
        "    def forward(self, logits):\n",
        "        return (logits.softmax(dim=-1) * self.proj).sum(dim=-1)\n",
        "\n",
        "class YoloV8LiteHead(nn.Module):\n",
        "    def __init__(self, in_channels_list, num_classes=80, hidden=256, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.reg_max = reg_max\n",
        "        self.integral = Integral(self.reg_max)\n",
        "        self.cls_towers = nn.ModuleList()\n",
        "        self.reg_towers = nn.ModuleList()\n",
        "        self.cls_preds = nn.ModuleList()\n",
        "        self.box_preds = nn.ModuleList()\n",
        "        \n",
        "        for in_ch in in_channels_list:\n",
        "            self.cls_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.reg_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.cls_preds.append(nn.Conv2d(hidden, num_classes, 1))\n",
        "            self.box_preds.append(nn.Conv2d(hidden, 4 * (self.reg_max + 1), 1))\n",
        "            \n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Bias Init: p=0.01 -> bias approx -4.6\n",
        "        bias = -4.595\n",
        "        for m in self.cls_preds:\n",
        "             nn.init.constant_(m.bias, bias)\n",
        "\n",
        "    def forward(self, features):\n",
        "        cls_outs = []\n",
        "        box_outs = []\n",
        "        for i, f in enumerate(features):\n",
        "            cls_outs.append(self.cls_preds[i](self.cls_towers[i](f)))\n",
        "            box_outs.append(self.box_preds[i](self.reg_towers[i](f)))\n",
        "        return cls_outs, box_outs\n",
        "\n",
        "class YoloModel(nn.Module):\n",
        "    def __init__(self, num_classes=80, backbone=\"yolov8_cspdarknet\", head_hidden=256, fpn_out=256):\n",
        "        super().__init__()\n",
        "        width = CFG.get(\"width\", 1.0)\n",
        "        depth = CFG.get(\"depth\", 1.0)\n",
        "        self.backbone = CSPDarknet(width=width, depth=depth)\n",
        "        base_c = [256, 512, 1024]\n",
        "        c3, c4, c5 = [int(x * width) for x in base_c]\n",
        "        self.neck = YOLOv8PAFPN(c3=c3, c4=c4, c5=c5, out_ch=fpn_out, width=width, depth=depth)\n",
        "        self.head = YoloV8LiteHead(in_channels_list=[c3, c4, c5], num_classes=num_classes, hidden=head_hidden, reg_max=CFG.get(\"reg_max\", 16))\n",
        "        self.strides = [8, 16, 32]\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        c3, c4, c5 = self.backbone(x)\n",
        "        p3, p4, p5 = self.neck(c3, c4, c5)\n",
        "        cls_outs, box_outs = self.head([p3, p4, p5])\n",
        "        head_out = {\"features\": [p3, p4, p5], \"cls\": cls_outs, \"box\": box_outs, \"strides\": self.strides}\n",
        "        \n",
        "        if self.training and targets is not None and hasattr(self, \"criterion\"):\n",
        "            losses, stats = self.criterion(head_out, targets)\n",
        "            return losses, stats\n",
        "        return head_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Loss Functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def bbox_iou(box1, box2, eps=1e-7):\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[..., 0], box1[..., 1], box1[..., 2], box1[..., 3]\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[..., 0], box2[..., 1], box2[..., 2], box2[..., 3]\n",
        "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
        "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
        "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
        "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
        "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1, min=0)\n",
        "    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
        "    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
        "    iou = inter_area / (b1_area + b2_area - inter_area + eps)\n",
        "    return iou\n",
        "\n",
        "def bbox_iou_xyxy_matrix(box1, box2, eps=1e-7):\n",
        "    # box1: [N, 4], box2: [M, 4]. Returns [N, M]\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
        "    \n",
        "    inter_rect_x1 = torch.max(b1_x1[:, None], b2_x1[None, :])\n",
        "    inter_rect_y1 = torch.max(b1_y1[:, None], b2_y1[None, :])\n",
        "    inter_rect_x2 = torch.min(b1_x2[:, None], b2_x2[None, :])\n",
        "    inter_rect_y2 = torch.min(b1_y2[:, None], b2_y2[None, :])\n",
        "    \n",
        "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1, min=0)\n",
        "    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
        "    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
        "    \n",
        "    iou = inter_area / (b1_area[:, None] + b2_area[None, :] - inter_area + eps)\n",
        "    return iou\n",
        "\n",
        "def make_grid(h, w, stride, device):\n",
        "    y, x = torch.meshgrid([torch.arange(h, device=device), torch.arange(w, device=device)], indexing='ij')\n",
        "    return (x.float() + 0.5) * stride, (y.float() + 0.5) * stride\n",
        "\n",
        "def decode_outputs(head_out, strides, conf_thres=0.001, max_det=300):\n",
        "    cls_outs = head_out[\"cls\"]\n",
        "    box_outs = head_out[\"box\"]\n",
        "    preds = []\n",
        "    \n",
        "    for b in range(cls_outs[0].shape[0]): # Batch\n",
        "        pred_b = []\n",
        "        for i, stride in enumerate(strides):\n",
        "            cls_Score = cls_outs[i][b].sigmoid()\n",
        "            if conf_thres > 0:\n",
        "                 mask = cls_Score > conf_thres\n",
        "                 if not mask.any(): continue\n",
        "            \n",
        "            # Optimization: Mask first, then decode\n",
        "            # But for simplicity in Phase 1, we decode all then filter (or simple threshold)\n",
        "            # Actually, standard is to flatten strides\n",
        "            pass \n",
        "            \n",
        "        # Simple flatten Decode\n",
        "        # (Reusing standard decode logic)\n",
        "        # ...\n",
        "        # Let's use a simpler implementation for 'QuickTest' correctness\n",
        "        pass\n",
        "    \n",
        "    # Re-using the known robust decode logic: \n",
        "    return decode_outputs_v8(head_out, strides, conf_thres, max_det)\n",
        "\n",
        "def decode_outputs_v8(head_out, strides, conf_thres, max_det):\n",
        "    device = head_out[\"cls\"][0].device\n",
        "    outputs = []\n",
        "    for b in range(head_out[\"cls\"][0].shape[0]):\n",
        "        batch_preds = []\n",
        "        for i, stride in enumerate(strides):\n",
        "             h, w = head_out[\"cls\"][i].shape[2:]\n",
        "             cx, cy = make_grid(h, w, stride, device)\n",
        "             cx, cy = cx.view(-1), cy.view(-1)\n",
        "             \n",
        "             c_out = head_out[\"cls\"][i][b].permute(1, 2, 0).reshape(-1, head_out[\"cls\"][0].shape[1]).sigmoid()\n",
        "             b_out = head_out[\"box\"][i][b].permute(1, 2, 0).reshape(-1, 4 * 16) # 16=dfl\n",
        "             \n",
        "             # Filter\n",
        "             mask = c_out.max(1)[0] > conf_thres\n",
        "             if not mask.any(): continue\n",
        "             \n",
        "             c_out = c_out[mask]\n",
        "             b_out = b_out[mask]\n",
        "             cx, cy = cx[mask], cy[mask]\n",
        "             \n",
        "             # Decode Box (DFL)\n",
        "             # We need DFL convolution logic or Expectation\n",
        "             # Standard DFL: softmax(0..15) * indices\n",
        "             b_len = b_out.shape[0]\n",
        "             b_out = b_out.view(b_len, 4, 16).softmax(3)\n",
        "             b_out = (b_out * torch.arange(16, device=device).float()).sum(3) * stride\n",
        "             \n",
        "             x1 = cx - b_out[:, 0]\n",
        "             y1 = cy - b_out[:, 1]\n",
        "             x2 = cx + b_out[:, 2]\n",
        "             y2 = cy + b_out[:, 3]\n",
        "             \n",
        "             boxes = torch.stack([x1, y1, x2, y2], 1)\n",
        "             scores, classes = c_out.max(1)\n",
        "             \n",
        "             # NMS preparation\n",
        "             # [x1, y1, x2, y2, score, class]\n",
        "             batch_preds.append(torch.cat([boxes, scores.unsqueeze(1), classes.float().unsqueeze(1)], 1))\n",
        "             \n",
        "        if len(batch_preds):\n",
        "             outputs.append(torch.cat(batch_preds, 0))\n",
        "        else:\n",
        "             outputs.append(torch.zeros(0, 6, device=device))\n",
        "             \n",
        "    # NMS\n",
        "    nms_outputs = []\n",
        "    for p in outputs:\n",
        "        if len(p) == 0: \n",
        "            nms_outputs.append(p)\n",
        "            continue\n",
        "        \n",
        "        # Sort\n",
        "        p = p[p[:, 4].argsort(descending=True)[:3000]]\n",
        "        \n",
        "        # Batched NMS\n",
        "        c = p[:, 5:6] * 7680\n",
        "        boxes, scores = p[:, :4] + c, p[:, 4]\n",
        "        i = torch.ops.torchvision.nms(boxes, scores, 0.7)\n",
        "        i = i[:max_det]\n",
        "        nms_outputs.append(p[i])\n",
        "        \n",
        "    return nms_outputs\n",
        "\n",
        "class DetectionLoss(nn.Module):\n",
        "    def __init__(self, num_classes, image_size=640, strides=[8, 16, 32], lambda_box=7.5, lambda_cls=0.5, lambda_dfl=1.5, dfl_ch=16):\n",
        "        super().__init__()\n",
        "        self.nc = num_classes\n",
        "        self.imgsz = image_size\n",
        "        self.strides = strides\n",
        "        self.lambda_box = lambda_box\n",
        "        self.lambda_cls = lambda_cls\n",
        "        self.lambda_dfl = lambda_dfl\n",
        "        self.dfl_ch = dfl_ch\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, head_out, targets):\n",
        "        cls_outs = head_out[\"cls\"]\n",
        "        box_outs = head_out[\"box\"]\n",
        "        device = cls_outs[0].device\n",
        "        B = cls_outs[0].shape[0]\n",
        "        \n",
        "        # Flatten and Cat all levels\n",
        "        pred_logits = torch.cat([x.permute(0, 2, 3, 1).reshape(B, -1, self.nc) for x in cls_outs], dim=1)\n",
        "        pred_dist = torch.cat([x.permute(0, 2, 3, 1).reshape(B, -1, 4 * self.dfl_ch) for x in box_outs], dim=1)\n",
        "        \n",
        "        # Build targets (Task Aligned)\n",
        "        targets_per_image, levels = self.build_targets(cls_outs, box_outs, targets)\n",
        "        \n",
        "        # Pre-calculate anchor grids for the whole batch\n",
        "        # (Actually we only need them for positive anchors, but computing once is faster)\n",
        "        all_grids_cx = []\n",
        "        all_grids_cy = []\n",
        "        all_strides = []\n",
        "        for lev in levels:\n",
        "            cx, cy = make_grid(lev[\"H\"], lev[\"W\"], lev[\"stride\"], device)\n",
        "            all_grids_cx.append(cx.flatten())\n",
        "            all_grids_cy.append(cy.flatten())\n",
        "            all_strides.append(torch.full((lev[\"H\"] * lev[\"W\"],), lev[\"stride\"], device=device, dtype=torch.float32))\n",
        "        \n",
        "        grids_cx = torch.cat(all_grids_cx)\n",
        "        grids_cy = torch.cat(all_grids_cy)\n",
        "        grids_strides = torch.cat(all_strides)\n",
        "        \n",
        "        loss_cls = torch.zeros(1, device=device)\n",
        "        loss_box = torch.zeros(1, device=device)\n",
        "        loss_dfl = torch.zeros(1, device=device)\n",
        "        total_norm = 0.0\n",
        "        num_pos_total = 0\n",
        "\n",
        "        for b in range(B):\n",
        "            t = targets_per_image[b]\n",
        "            pos_idx = t[\"pos_index\"]\n",
        "            num_pos = pos_idx.numel()\n",
        "            num_pos_total += num_pos\n",
        "            \n",
        "            # Update Norm (sum of target scores)\n",
        "            # If nothing found, norm for this image is 0 (handled by max(total_norm, 1.0) at end)\n",
        "            t_norm = t.get(\"target_scores_sum\", 0.0)\n",
        "            total_norm += t_norm\n",
        "\n",
        "            # --- Classification Loss ---\n",
        "            target_cls = torch.zeros_like(pred_logits[b])\n",
        "            if num_pos > 0:\n",
        "                target_cls[pos_idx] = t[\"t_cls_soft\"].to(target_cls.dtype)\n",
        "            loss_cls += self.bce(pred_logits[b], target_cls).sum()\n",
        "\n",
        "            # --- Box Loss (Positive only) ---\n",
        "            if num_pos > 0:\n",
        "                # Important: YOLOv8 weights box loss by alignment scores\n",
        "                target_scores = t[\"t_cls_soft\"].sum(-1) # [num_pos]\n",
        "                \n",
        "                # DFL Loss weighting\n",
        "                p_dist_pos = pred_dist[b][pos_idx].view(-1, 4, self.dfl_ch)\n",
        "                t_ltrb_bins = t[\"t_box_ltrb\"] / grids_strides[pos_idx].unsqueeze(-1)\n",
        "                t_ltrb_bins = t_ltrb_bins.clamp(0, self.dfl_ch - 1.01)\n",
        "                \n",
        "                tl = t_ltrb_bins.long()\n",
        "                tr = tl + 1\n",
        "                wl = tr.float() - t_ltrb_bins\n",
        "                wr = t_ltrb_bins - tl.float()\n",
        "                \n",
        "                l_dfl = (F.cross_entropy(p_dist_pos.view(-1, self.dfl_ch), tl.view(-1), reduction=\"none\").view(-1, 4) * wl +\n",
        "                         F.cross_entropy(p_dist_pos.view(-1, self.dfl_ch), tr.view(-1), reduction=\"none\").view(-1, 4) * wr).mean(-1)\n",
        "                loss_dfl += (l_dfl * target_scores).sum()\n",
        "\n",
        "                # IoU Loss weighting\n",
        "                p_ltrb_bins = (p_dist_pos.softmax(-1) * torch.arange(self.dfl_ch, device=device).float()).sum(-1)\n",
        "                p_ltrb_pixels = p_ltrb_bins * grids_strides[pos_idx].unsqueeze(-1)\n",
        "                \n",
        "                cx, cy = grids_cx[pos_idx], grids_cy[pos_idx]\n",
        "                p_xyxy = torch.stack([cx - p_ltrb_pixels[:, 0], cy - p_ltrb_pixels[:, 1],\n",
        "                                      cx + p_ltrb_pixels[:, 2], cy + p_ltrb_pixels[:, 3]], dim=-1)\n",
        "                \n",
        "                iou = bbox_iou(p_xyxy, t[\"t_box_xyxy\"])\n",
        "                loss_box += ((1.0 - iou).squeeze(-1) * target_scores).sum()\n",
        "\n",
        "        # Final Normalization (Target Score Summation)\n",
        "        norm = max(total_norm, 1.0)\n",
        "        loss_cls = (loss_cls * self.lambda_cls) / norm\n",
        "        loss_box = (loss_box * self.lambda_box) / norm\n",
        "        loss_dfl = (loss_dfl * self.lambda_dfl) / norm\n",
        "        \n",
        "        return {\"loss\": loss_cls + loss_box + loss_dfl, \"loss_cls\": loss_cls, \"loss_box\": loss_box, \"loss_dfl\": loss_dfl}, {\"num_pos\": num_pos_total, \"norm\": norm}\n",
        "\n",
        "    def build_targets(self, cls_outs, box_outs, targets):\n",
        "        gt_classes = targets[\"labels\"]\n",
        "        gt_boxes = targets[\"boxes\"]\n",
        "        batch_idx = targets[\"batch_index\"]\n",
        "        B = cls_outs[0].shape[0]\n",
        "        gt_cls_list = []\n",
        "        gt_box_list = []\n",
        "        for i in range(B):\n",
        "            mask = batch_idx == i\n",
        "            gt_cls_list.append(gt_classes[mask])\n",
        "            gt_box_list.append(gt_boxes[mask])\n",
        "        return build_targets_task_aligned(cls_outs, box_outs, self.strides, gt_cls_list, gt_box_list, self.imgsz)\n",
        "\n",
        "def build_targets_task_aligned(cls_outs, box_outs, strides, gt_classes, gt_boxes_xyxy, image_size):\n",
        "    device = cls_outs[0].device\n",
        "    B = cls_outs[0].shape[0]\n",
        "    C = cls_outs[0].shape[1]\n",
        "    levels = []\n",
        "    start = 0\n",
        "    grids = []\n",
        "    for (cl, s) in zip(cls_outs, strides):\n",
        "        _, _, H, W = cl.shape\n",
        "        levels.append({\"H\": H, \"W\": W, \"stride\": s, \"start\": start, \"end\": start + H * W})\n",
        "        cx, cy = make_grid(H, W, s, device)\n",
        "        grids.append((cx, cy))\n",
        "        start += H * W\n",
        "        \n",
        "    tal_alpha, tal_beta, tal_topk, tal_cr = 0.5, 6.0, 10, 2.5\n",
        "    \n",
        "    per_image_targets = []\n",
        "    for b in range(B):\n",
        "        cls_per_image = [cl[b].permute(1, 2, 0).reshape(-1, C) for cl in cls_outs]\n",
        "        cls_flat = torch.cat(cls_per_image, dim=0)\n",
        "        N_total = cls_flat.shape[0]\n",
        "        gtc = gt_classes[b]\n",
        "        gtb = gt_boxes_xyxy[b]\n",
        "        Ng = int(gtc.numel())\n",
        "        if Ng == 0:\n",
        "            per_image_targets.append({\"t_cls_soft\": torch.zeros(0, C, device=device), \"t_box_xyxy\": torch.zeros(0, 4, device=device), \"t_box_ltrb\": torch.zeros(0, 4, device=device), \"pos_index\": torch.zeros(0, dtype=torch.long, device=device), \"target_scores_sum\": 0.0})\n",
        "            continue\n",
        "        \n",
        "        pred_xyxy_levels = []\n",
        "        for (bx, level, (cx, cy)) in zip(box_outs, levels, grids):\n",
        "            H, W, s = level[\"H\"], level[\"W\"], level[\"stride\"]\n",
        "            bl = bx[b]\n",
        "            M1 = bl.shape[0] // 4\n",
        "            bl = bl.view(4, M1, H, W).permute(2, 3, 0, 1).reshape(H * W, 4, M1)\n",
        "            probs = bl.softmax(dim=-1)\n",
        "            proj = torch.arange(M1, device=device, dtype=bl.dtype)\n",
        "            dists = (probs * proj).sum(dim=-1) * float(s)\n",
        "            x1, y1 = cx - dists[:, 0], cy - dists[:, 1]\n",
        "            x2, y2 = cx + dists[:, 2], cy + dists[:, 3]\n",
        "            pred_xyxy_levels.append(torch.stack([x1, y1, x2, y2], dim=-1).clamp_(0, image_size))\n",
        "        pred_xyxy = torch.cat(pred_xyxy_levels, dim=0)\n",
        "\n",
        "        candidate_mask = torch.zeros(N_total, Ng, dtype=torch.bool, device=device)\n",
        "        for level, (cx, cy) in enumerate(grids):\n",
        "            start, end, s = levels[level][\"start\"], levels[level][\"end\"], levels[level][\"stride\"]\n",
        "            Nl = end - start\n",
        "            cxv, cyv = cx.view(Nl, 1), cy.view(Nl, 1)\n",
        "            gt_centers = 0.5 * (gtb[:, :2] + gtb[:, 2:])\n",
        "            half = tal_cr * s\n",
        "            in_center = (cxv >= gt_centers[:, 0] - half) & (cyv >= gt_centers[:, 1] - half) & (cxv <= gt_centers[:, 0] + half) & (cyv <= gt_centers[:, 1] + half)\n",
        "            candidate_mask[start:end] |= in_center\n",
        "        \n",
        "        cls_sigmoid = cls_flat.sigmoid()\n",
        "        cls_gt_scores = cls_sigmoid[:, gtc]\n",
        "        iou_matrix = box_iou_xyxy_matrix(pred_xyxy, gtb)\n",
        "        align = (cls_gt_scores.clamp(min=1e-9).pow(tal_alpha)) * (iou_matrix.clamp(min=1e-9).pow(tal_beta))\n",
        "        align = torch.where(candidate_mask, align, torch.full_like(align, -1e-9))\n",
        "        \n",
        "        k = min(tal_topk, align.shape[0])\n",
        "        topk_scores, topk_index = torch.topk(align, k, dim=0)\n",
        "        best_gt_per_pred = torch.full((N_total,), -1, dtype=torch.long, device=device)\n",
        "        best_score_per_pred = torch.full((N_total,), -1e-9, dtype=align.dtype, device=device)\n",
        "        for j in range(Ng):\n",
        "            idx_j = topk_index[:, j]\n",
        "            score_j = topk_scores[:, j]\n",
        "            better = score_j > best_score_per_pred[idx_j]\n",
        "            best_gt_per_pred[idx_j[better]] = j\n",
        "            best_score_per_pred[idx_j[better]] = score_j[better]\n",
        "            \n",
        "        pos_mask = best_gt_per_pred >= 0\n",
        "        pos_index = torch.nonzero(pos_mask, as_tuple=False).squeeze(1)\n",
        "        \n",
        "        if pos_index.numel() == 0:\n",
        "            per_image_targets.append({\"t_cls_soft\": torch.zeros(0, C, device=device), \"t_box_xyxy\": torch.zeros(0, 4, device=device), \"t_box_ltrb\": torch.zeros(0, 4, device=device), \"pos_index\": pos_index, \"target_scores_sum\": 0.0})\n",
        "            continue\n",
        "            \n",
        "        gt_index = best_gt_per_pred[pos_index]\n",
        "        scores = best_score_per_pred[pos_index].clamp(min=0.0)\n",
        "        t_cls_soft = torch.zeros(len(pos_index), C, device=device)\n",
        "        t_cls_soft[torch.arange(len(pos_index)), gtc[gt_index]] = scores\n",
        "        t_box_xyxy = gtb[gt_index]\n",
        "        \n",
        "        t_box_ltrb = torch.empty(len(pos_index), 4, device=device)\n",
        "        for level_i, level in enumerate(levels):\n",
        "            start, end, s = level[\"start\"], level[\"end\"], level[\"stride\"]\n",
        "            cx, cy = grids[level_i]\n",
        "            in_level = (pos_index >= start) & (pos_index < end)\n",
        "            if in_level.any():\n",
        "                idx_l = pos_index[in_level] - start\n",
        "                ct = torch.stack((cx[idx_l], cy[idx_l]), dim=-1)\n",
        "                gs = gtb[gt_index[in_level]]\n",
        "                t_box_ltrb[in_level] = torch.stack((ct[:,0]-gs[:,0], ct[:,1]-gs[:,1], gs[:,2]-ct[:,0], gs[:,3]-ct[:,1]), dim=-1).clamp(min=0, max=float(image_size))\n",
        "        \n",
        "        per_image_targets.append({\n",
        "            \"t_cls_soft\": t_cls_soft,\n",
        "            \"t_box_xyxy\": t_box_xyxy,\n",
        "            \"t_box_ltrb\": t_box_ltrb,\n",
        "            \"pos_index\": pos_index,\n",
        "            \"target_scores_sum\": scores.sum().item()\n",
        "        })\n",
        "    return per_image_targets, levels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Dataset (Mosaic + Affine)\n",
        "def random_perspective(im, targets=(), degrees=10, translate=.1, scale=.1, shear=10, perspective=0.0, border=(0, 0)):\n",
        "    height, width = im.shape[:2]\n",
        "    \n",
        "    # Center\n",
        "    C = np.eye(3)\n",
        "    C[0, 2] = -width / 2\n",
        "    C[1, 2] = -height / 2\n",
        "    \n",
        "    # Perspective\n",
        "    P = np.eye(3)\n",
        "    P[2, 0] = random.uniform(-perspective, perspective)\n",
        "    P[2, 1] = random.uniform(-perspective, perspective)\n",
        "    \n",
        "    # Rotation and Scale\n",
        "    R = np.eye(3)\n",
        "    a = random.uniform(-degrees, degrees)\n",
        "    s = random.uniform(1 - scale, 1 + scale)\n",
        "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
        "    \n",
        "    # Shear\n",
        "    S = np.eye(3)\n",
        "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)\n",
        "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)\n",
        "    \n",
        "    # Translation\n",
        "    T = np.eye(3)\n",
        "    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width\n",
        "    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height\n",
        "    \n",
        "    # Combined rotation matrix\n",
        "    M = T @ S @ R @ P @ C\n",
        "    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():\n",
        "        if perspective:\n",
        "            im = cv2.warpPerspective(im, M, dsize=(width, height), borderValue=(114, 114, 114))\n",
        "        else:\n",
        "            im = cv2.warpAffine(im, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\n",
        "            \n",
        "    # Transform targets\n",
        "    n = len(targets)\n",
        "    if n:\n",
        "        xy = np.ones((n * 4, 3))\n",
        "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
        "        xy = xy @ M.T  # transform\n",
        "        xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n",
        "        \n",
        "        # Create new boxes\n",
        "        x = xy[:, [0, 2, 4, 6]]\n",
        "        y = xy[:, [1, 3, 5, 7]]\n",
        "        new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
        "        \n",
        "        # Clip\n",
        "        new[:, [0, 2]] = new[:, [0, 2]].clip(0, width)\n",
        "        new[:, [1, 3]] = new[:, [1, 3]].clip(0, height)\n",
        "        \n",
        "        # Filter candidates\n",
        "        i = box_candidates(targets[:, 1:5].T * s, new.T, area_thr=0.10) # 0.10 area thr\n",
        "        targets = targets[i]\n",
        "        targets[:, 1:5] = new[i]\n",
        "        \n",
        "    return im, targets\n",
        "\n",
        "def box_candidates(box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):\n",
        "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
        "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
        "    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))\n",
        "    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, imgsz=640, augment=True, pad_value=114):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.imgsz = imgsz\n",
        "        self.augment = augment\n",
        "        self.pad_value = pad_value\n",
        "        \n",
        "        # Scan files\n",
        "        if os.path.isdir(image_dir):\n",
        "            self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")) + glob.glob(os.path.join(image_dir, \"*.png\")))\n",
        "        else:\n",
        "             # Assume it's a list (not implemented fully here but safe fallback)\n",
        "             self.image_paths = []\n",
        "             \n",
        "        self.label_paths = []\n",
        "        for img_path in self.image_paths:\n",
        "             base = os.path.basename(img_path)\n",
        "             name = os.path.splitext(base)[0]\n",
        "             lbl_path = os.path.join(self.label_dir, name + \".txt\")\n",
        "             self.label_paths.append(lbl_path)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def load_image(self, index):\n",
        "        # Load and resize to imgsz with padding (letterbox style essentially but simpler here)\n",
        "        path = self.image_paths[index]\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "             print(f\"âŒ Failed to load {path}\")\n",
        "             return np.full((self.imgsz, self.imgsz, 3), 114, dtype=np.uint8), (0, 0), (1, 1)\n",
        "             \n",
        "        h, w = img.shape[:2]\n",
        "        r = min(self.imgsz / h, self.imgsz / w)\n",
        "        new_h, new_w = int(h * r), int(w * r)\n",
        "        resized = cv2.resize(img, (new_w, new_h))\n",
        "        \n",
        "        return resized, (h, w), (new_h, new_w)\n",
        "\n",
        "    def load_mosaic(self, index):\n",
        "        # YOLO Mosaic Augmentation\n",
        "        labels4 = []\n",
        "        s = self.imgsz\n",
        "        yc, xc = (int(random.uniform(-x, 2 * s + x)) for x in [-s // 2])  # mosaic center x, y\n",
        "        indices = [index] + random.choices(range(len(self)), k=3)  # 3 additional images\n",
        "        random.shuffle(indices)\n",
        "        \n",
        "        result_img = np.full((s * 2, s * 2, 3), self.pad_value, dtype=np.uint8)\n",
        "        \n",
        "        for i, idx in enumerate(indices):\n",
        "            img, _, (h, w) = self.load_image(idx)\n",
        "            \n",
        "            # Place coordinates\n",
        "            if i == 0:  # top left\n",
        "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc\n",
        "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h\n",
        "            elif i == 1:  # top right\n",
        "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
        "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
        "            elif i == 2:  # bottom left\n",
        "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(yc + h, s * 2)\n",
        "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n",
        "            elif i == 3:  # bottom right\n",
        "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(yc + h, s * 2)\n",
        "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
        "                \n",
        "            result_img[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]\n",
        "            padw = x1a - x1b\n",
        "            padh = y1a - y1b\n",
        "            \n",
        "            # Labels\n",
        "            lbl_path = self.label_paths[idx]\n",
        "            if os.path.exists(lbl_path):\n",
        "                 with open(lbl_path, \"r\") as f:\n",
        "                     l = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)\n",
        "                 if len(l):\n",
        "                     # xywh to xyxy\n",
        "                     h_orig, w_orig = self.load_image(idx)[1] # get original size for norm\n",
        "                     # Normalized xywh -> pixel xyxy\n",
        "                     boxes = l[:, 1:5]\n",
        "                     boxes[:, [0, 2]] *= w_orig\n",
        "                     boxes[:, [1, 3]] *= h_orig\n",
        "                     # Scale to current resized dims\n",
        "                     scale_w = w / w_orig\n",
        "                     scale_h = h / h_orig\n",
        "                     boxes[:, [0, 2]] *= scale_w\n",
        "                     boxes[:, [1, 3]] *= scale_h\n",
        "                     \n",
        "                     # Convert xywh to xyxy\n",
        "                     b = boxes.copy()\n",
        "                     b[:, 0] = boxes[:, 0] - boxes[:, 2] / 2\n",
        "                     b[:, 1] = boxes[:, 1] - boxes[:, 3] / 2\n",
        "                     b[:, 2] = boxes[:, 0] + boxes[:, 2] / 2\n",
        "                     b[:, 3] = boxes[:, 1] + boxes[:, 3] / 2\n",
        "                     boxes = b\n",
        "                     \n",
        "                     boxes[:, 0] += padw\n",
        "                     boxes[:, 1] += padh\n",
        "                     boxes[:, 2] += padw\n",
        "                     boxes[:, 3] += padh\n",
        "                     \n",
        "                     l[:, 1:5] = boxes\n",
        "                     labels4.append(l)\n",
        "        \n",
        "        if len(labels4):\n",
        "            labels4 = np.concatenate(labels4, 0)\n",
        "            np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])\n",
        "        else:\n",
        "            labels4 = np.zeros((0, 5))\n",
        "            \n",
        "        # Augment (Center Crop 640 from 1280 or Resize?)\n",
        "        # We crop a random 640x640 patch or resize? Standard is resize+crop. \n",
        "        # For simplicity here: Resize result to imgsz\n",
        "        final_img = cv2.resize(result_img, (self.imgsz, self.imgsz))\n",
        "        # Adjust labels\n",
        "        scale = self.imgsz / (2 * s)\n",
        "        labels4[:, 1:] *= scale\n",
        "        \n",
        "        return final_img, labels4\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.augment:\n",
        "            # Mosaic\n",
        "            img, labels = self.load_mosaic(index)\n",
        "            # HSV (Optional - keep simple for now)\n",
        "            # Affine\n",
        "            img, labels = random_perspective(img, labels, border=(0, 0))\n",
        "        else:\n",
        "            # Standard Load (Validation)\n",
        "            img, (h_o, w_o), (h, w) = self.load_image(index)\n",
        "            # Letterbox padding if needed\n",
        "            top = (self.imgsz - h) // 2\n",
        "            left = (self.imgsz - w) // 2\n",
        "            img = cv2.copyMakeBorder(img, top, self.imgsz-h-top, left, self.imgsz-w-left, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "            # Load Labels\n",
        "            lbl_path = self.label_paths[index]\n",
        "            labels = np.zeros((0, 5))\n",
        "            if os.path.exists(lbl_path):\n",
        "                 with open(lbl_path, \"r\") as f:\n",
        "                     l = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)\n",
        "                 if len(l):\n",
        "                     # Normalized xywh -> target xyxy (padded)\n",
        "                     b = l[:, 1:5].copy()\n",
        "                     b[:, 0] = l[:, 1] * w_o * (w/w_o) + left # x_center in new img\n",
        "                     b[:, 1] = l[:, 2] * h_o * (h/h_o) + top\n",
        "                     b[:, 2] = l[:, 3] * w_o * (w/w_o)\n",
        "                     b[:, 3] = l[:, 4] * h_o * (h/h_o)\n",
        "                     # xywh to xyxy\n",
        "                     l[:, 1] = b[:, 0] - b[:, 2] / 2\n",
        "                     l[:, 2] = b[:, 1] - b[:, 3] / 2\n",
        "                     l[:, 3] = b[:, 0] + b[:, 2] / 2\n",
        "                     l[:, 4] = b[:, 1] + b[:, 3] / 2\n",
        "                     labels = l\n",
        "\n",
        "        # Normalize & Torch\n",
        "        img = img.transpose(2, 0, 1)[::-1] # BGR to RGB -> HWC to CHW\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).float() / 255.0\n",
        "        \n",
        "        num_targets = len(labels)\n",
        "        if num_targets > 0:\n",
        "            # Cls, x1, y1, x2, y2\n",
        "            out_labels = torch.from_numpy(labels[:, 0])\n",
        "            out_boxes = torch.from_numpy(labels[:, 1:])\n",
        "        else:\n",
        "            out_labels = torch.zeros(0)\n",
        "            out_boxes = torch.zeros(0, 4)\n",
        "            \n",
        "        return img, {\"labels\": out_labels, \"boxes\": out_boxes, \"index\": torch.tensor(index)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Dataset Management Logic\n",
        "COCO_CLASSES = [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "    'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "def ensure_dataset_ready(size=5000):\n",
        "    \"\"\"\n",
        "    Ensures the dataset is downloaded and exported to YOLO format in CFG['data_root'].\n",
        "    Uses a .complete sentinel file to handle interruptions during export.\n",
        "    \"\"\"\n",
        "    out_dir = CFG[\"data_root\"]\n",
        "    sentinel = os.path.join(out_dir, \".complete\")\n",
        "    \n",
        "    if os.path.exists(sentinel):\n",
        "        print(f\"âœ… Fixed dataset found and complete at {out_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nðŸ“¦ Preparing Fixed Dataset (Size: {size})...\")\n",
        "    \n",
        "    # If it was interrupted, clean and restart\n",
        "    if os.path.exists(out_dir):\n",
        "        shutil.rmtree(out_dir)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    train_name = \"coco_fixed_train\"\n",
        "    val_name = \"coco_fixed_val\"\n",
        "    \n",
        "    # Cleanup fiftyone datasets with these names if they exist\n",
        "    if train_name in fo.list_datasets(): fo.delete_dataset(train_name)\n",
        "    if val_name in fo.list_datasets(): fo.delete_dataset(val_name)\n",
        "\n",
        "    # 2. Download/Load from Zoo\n",
        "    print(\"Downloading train split...\")\n",
        "    train_ds = foz.load_zoo_dataset(\n",
        "        \"coco-2017\",\n",
        "        split=\"train\", \n",
        "        label_types=[\"detections\"],\n",
        "        max_samples=size,\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        dataset_name=train_name\n",
        "    )\n",
        "\n",
        "    print(\"Downloading val split...\")\n",
        "    val_ds = foz.load_zoo_dataset(\n",
        "        \"coco-2017\",\n",
        "        split=\"validation\", \n",
        "        label_types=[\"detections\"],\n",
        "        max_samples=500,\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        dataset_name=val_name\n",
        "    )\n",
        "    \n",
        "    # 3. Export to YOLO format\n",
        "    print(f\"Exporting to {out_dir}...\")\n",
        "    train_ds.export(\n",
        "        export_dir=out_dir,\n",
        "        dataset_type=fo.types.YOLOv5Dataset,\n",
        "        label_field=\"ground_truth\",\n",
        "        split=\"train\",\n",
        "        classes=COCO_CLASSES\n",
        "    )\n",
        "    \n",
        "    val_ds.export(\n",
        "        export_dir=out_dir,\n",
        "        dataset_type=fo.types.YOLOv5Dataset,\n",
        "        label_field=\"ground_truth\",\n",
        "        split=\"val\",\n",
        "        classes=COCO_CLASSES\n",
        "    )\n",
        "    \n",
        "    # Create sentinel file\n",
        "    with open(sentinel, \"w\") as f:\n",
        "        f.write(str(datetime.now()))\n",
        "    \n",
        "    # Cleanup from fiftyone memory\n",
        "    fo.delete_dataset(train_name)\n",
        "    fo.delete_dataset(val_name)\n",
        "    \n",
        "    print(f\"âœ… Dataset export complete at {out_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Initial Initialization ---\n",
        "start_epoch = 0\n",
        "global_step = 0\n",
        "last_ckpt_path = os.path.join(RUN_DIR, \"last.pt\")\n",
        "metrics_csv = os.path.join(RUN_DIR, \"metrics.csv\")\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "model = YOLOv8(nc=CFG['num_classes'], width=CFG['width'], depth=CFG['depth']).to(device)\n",
        "ema = ModelEMA(model, decay=CFG['ema_decay'])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
        "scaler = GradScaler('cuda', enabled=CFG['amp'])\n",
        "\n",
        "# --- Resume Logic ---\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    print(f\"ðŸ”„ Resuming from {last_ckpt_path}...\")\n",
        "    ckpt = torch.load(last_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    ema.ema.load_state_dict(ckpt['ema'])\n",
        "    ema.updates = ckpt.get('ema_updates', 0)\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    scaler.load_state_dict(ckpt['scaler'])\n",
        "    start_epoch = ckpt['epoch']\n",
        "    global_step = ckpt['global_step']\n",
        "    # Scheduler will be handled after initialization to match the steps\n",
        "\n",
        "# --- Scheduler & Dataset ---\n",
        "def get_scheduler(opt, total_steps):\n",
        "    warmup_steps = int(total_steps * 0.05)\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps: return float(step) / float(max(1, warmup_steps))\n",
        "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return 0.5 * (1.0 + math.cos(progress * math.pi))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "# Ensure data is ready\n",
        "ensure_dataset_ready(size=NUM_SAMPLES)\n",
        "\n",
        "train_ds = YoloDataset(os.path.join(CFG[\"data_root\"], \"images/train\"), \n",
        "                       os.path.join(CFG[\"data_root\"], \"labels/train\"), imgsz=CFG[\"imgsz\"])\n",
        "val_ds = YoloDataset(os.path.join(CFG[\"data_root\"], \"images/val\"), \n",
        "                     os.path.join(CFG[\"data_root\"], \"labels/val\"), imgsz=CFG[\"imgsz\"], augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=CFG[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "total_training_steps = len(train_loader) * CFG['epochs']\n",
        "scheduler = get_scheduler(optimizer, total_training_steps)\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    scheduler.load_state_dict(ckpt['scheduler'])\n",
        "\n",
        "start_time_global = time.time()\n",
        "\n",
        "print(f\"\\nðŸš€ Starting Fixed-Dataset training from Epoch {start_epoch+1}/{CFG['epochs']}\")\n",
        "print(f\"ðŸ“Š Total training steps: {total_training_steps}\")\n",
        "\n",
        "# --- Main Loop (Epoch Based) ---\n",
        "try:\n",
        "    for epoch in range(start_epoch, CFG['epochs']):\n",
        "        if (time.time() - start_time_global) > CFG['time_limit']:\n",
        "             print(\"â° Time Limit Reached! Stopping safely.\")\n",
        "             save_checkpoint(last_ckpt_path, 0, epoch, global_step, complete=False)\n",
        "             break\n",
        "             \n",
        "        model.train()\n",
        "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{CFG['epochs']}\")\n",
        "        for i, (imgs, targets) in pbar:\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            for k, v in targets.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    targets[k] = v.to(device, non_blocking=True)\n",
        "            \n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast(\"cuda\", enabled=CFG[\"amp\"]):\n",
        "                head_out = model(imgs)\n",
        "                losses, stats = model.criterion(head_out, targets)\n",
        "                total_loss = losses[\"loss\"]\n",
        "            \n",
        "            scaler.scale(total_loss).backward()\n",
        "            if CFG.get(\"grad_clip_norm\"):\n",
        "                 scaler.unscale_(optimizer)\n",
        "                 torch.nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip_norm\"])\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            ema.update(model)\n",
        "            global_step += 1\n",
        "            \n",
        "            if i % 10 == 0:\n",
        "                lr = optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix({\"loss\": f\"{total_loss.item():.2f}\", \"lr\": f\"{lr:.2e}\"})\n",
        "            \n",
        "            if i > 0 and i % 100 == 0:\n",
        "                 visualize_inline(model, imgs, targets, global_step, device, COCO_CLASSES)\n",
        "            \n",
        "            # Optional: Intermediate checkpoint every few iterations if it's a long epoch\n",
        "            if i > 0 and i % 500 == 0:\n",
        "                 save_checkpoint(last_ckpt_path, 0, epoch, global_step)\n",
        "\n",
        "        scheduler.step()\n",
        "        map50 = validate(ema.ema, val_loader, device)\n",
        "        \n",
        "        # Save Metrics\n",
        "        with open(metrics_csv, \"a\") as f:\n",
        "            if os.stat(metrics_csv).st_size == 0:\n",
        "                f.write(\"epoch,map50,time\\n\")\n",
        "            f.write(f\"{epoch+1},{map50:.4f},{time.time()-start_time_global:.1f}\\n\")\n",
        "        \n",
        "        # Final checkpoint for epoch\n",
        "        save_checkpoint(last_ckpt_path, 0, epoch + 1, global_step)\n",
        "        \n",
        "    print(\"ðŸ Training Finished!\")\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nðŸ›‘ Interrupted by user!\")\n",
        "    save_checkpoint(last_ckpt_path, 0, epoch, global_step)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Fatal Error: {e}\")\n",
        "    save_checkpoint(os.path.join(RUN_DIR, \"error_state.pt\"), 0, epoch, global_step)\n",
        "    raise e\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
