{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Imports & Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import copy\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "# Set Seed\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"‚úÖ Notebook Updated for Fixed Dataset Training\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Configuration\n",
        "# --- USER SETTINGS ---\n",
        "QUICK_TEST = True  # Set to True for a fast smoke test\n",
        "NUM_SAMPLES = 1000 if QUICK_TEST else 20000  # Total images on disk\n",
        "TOTAL_EPOCHS = 3 if QUICK_TEST else 100     # Training goal\n",
        "\n",
        "BASE_DIR = os.path.abspath(\"yolo-lab\")\n",
        "DIRS = {\n",
        "    \"datasets\": os.path.join(BASE_DIR, \"datasets\"),\n",
        "    \"runs\": os.path.join(BASE_DIR, \"runs\"),\n",
        "    \"configs\": os.path.join(BASE_DIR, \"configs\"),\n",
        "}\n",
        "for d in DIRS.values():\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "EXP_NAME = \"yolov8_fixed_dataset\"\n",
        "RUN_NAME = EXP_NAME # Stable run name\n",
        "if QUICK_TEST: RUN_NAME += f\"_test_{timestamp}\"\n",
        "RUN_DIR = os.path.join(DIRS[\"runs\"], RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "# Model & Training Config\n",
        "CFG = {\n",
        "    \"exp_name\": EXP_NAME,\n",
        "    \"run_name\": RUN_NAME,\n",
        "    \"seed\": 42,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch_size\": 8 if QUICK_TEST else 16,\n",
        "    \"num_classes\": 80,\n",
        "    \"time_limit\": 36000, # 10 hours default\n",
        "    \n",
        "    # Model\n",
        "    \"width\": 1.0,\n",
        "    \"depth\": 1.0,\n",
        "    \"reg_max\": 16,\n",
        "    \"head_hidden\": 256,\n",
        "    \"backbone\": \"yolov8_cspdarknet\",\n",
        "    \n",
        "    # Optimizer\n",
        "    \"optimizer\": \"adamw\",\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"cosine_schedule\": True,\n",
        "    \"epochs\": TOTAL_EPOCHS,\n",
        "    \"amp\": True,\n",
        "    \"grad_clip_norm\": 10.0,\n",
        "    \"ema_decay\": 0.9998,\n",
        "    \n",
        "    # Loss\n",
        "    \"tal_alpha\": 1.0,\n",
        "    \"tal_beta\": 6.0,\n",
        "    \"tal_topk\": 10,\n",
        "    \"tal_center_radius\": 2.5,\n",
        "    \"loss_weights\": {\"box\": 7.5, \"cls\": 0.5, \"dfl\": 1.5},\n",
        "    \n",
        "    # Augmentation\n",
        "    \"letterbox_pad\": 114,\n",
        "    \"hflip_p\": 0.5,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.7,\n",
        "    \"hsv_v\": 0.4,\n",
        "    \"grad_clip_norm\": 10.0,\n",
        "    \"accumulate\": 1, \n",
        "    \"min_lr_ratio\": 0.05,\n",
        "    \"pretrained\": None,\n",
        "    \n",
        "    # Paths (Fixed local dataset)\n",
        "    \"data_root\": os.path.join(DIRS[\"datasets\"], \"coco_fixed\"),\n",
        "    \"train_img_dir\": \"images/train\",\n",
        "    \"train_lbl_dir\": \"labels/train\",\n",
        "    \"val_img_dir\": \"images/val\",\n",
        "    \"val_lbl_dir\": \"labels/val\",\n",
        "}\n",
        "\n",
        "print(\"Run Directory:\", RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Cleanup Utilities\n",
        "def confirm_action(prompt):\n",
        "    ans = input(f\"{prompt} (type 'yes' to confirm): \")\n",
        "    return ans.lower() == 'yes'\n",
        "\n",
        "def clear_last_checkpoint():\n",
        "    \"\"\"Deletes last.pt in the current RUN_DIR\"\"\"\n",
        "    path = os.path.join(RUN_DIR, \"last.pt\")\n",
        "    if os.path.exists(path):\n",
        "        if confirm_action(f\"Clean checkpoint at {path}?\"):\n",
        "            os.remove(path)\n",
        "            print(f\"üóëÔ∏è Deleted {path}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è No checkpoint found at {path}\")\n",
        "\n",
        "def wipe_photos():\n",
        "    \"\"\"Deletes the images folder in data_root\"\"\"\n",
        "    path = os.path.join(CFG[\"data_root\"], \"images\")\n",
        "    if os.path.exists(path):\n",
        "        if confirm_action(f\"WIPE ALL PHOTOS in {path}?\"):\n",
        "            shutil.rmtree(path)\n",
        "            print(f\"üóëÔ∏è Deleted {path}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è No photos found at {path}\")\n",
        "\n",
        "def wipe_data():\n",
        "    \"\"\"Deletes the labels folder in data_root\"\"\"\n",
        "    path = os.path.join(CFG[\"data_root\"], \"labels\")\n",
        "    if os.path.exists(path):\n",
        "        if confirm_action(f\"WIPE ALL DATA (labels) in {path}?\"):\n",
        "            shutil.rmtree(path)\n",
        "            print(f\"üóëÔ∏è Deleted {path}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è No data found at {path}\")\n",
        "\n",
        "def full_wipe():\n",
        "    \"\"\"Wipes the entire yolo-lab project directory (runs and datasets)\"\"\"\n",
        "    if confirm_action(\"DANGER: WIPE EVERYTHING (runs, datasets, configs)?\"):\n",
        "        if os.path.exists(BASE_DIR):\n",
        "            shutil.rmtree(BASE_DIR)\n",
        "            print(f\"üóëÔ∏è Deleted {BASE_DIR}\")\n",
        "            # Re-create structure for safety\n",
        "            for d in DIRS.values():\n",
        "                os.makedirs(d, exist_ok=True)\n",
        "            os.makedirs(RUN_DIR, exist_ok=True)\n",
        "            print(\"‚úÖ Structure re-initialized.\")\n",
        "\n",
        "print(\"‚õëÔ∏è Cleanup Utils Loaded: clear_last_checkpoint(), wipe_photos(), wipe_data(), full_wipe()\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Model Architecture\n",
        "def autopad(k, p=None, d=1):\n",
        "    if d > 1:\n",
        "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]\n",
        "    if p is None:\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n",
        "    return p\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    default_act = nn.SiLU()\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
        "        super().__init__()\n",
        "        c_ = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
        "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "class C2f(nn.Module):\n",
        "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
        "        super().__init__()\n",
        "        self.c = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
        "        self.cv2 = Conv((2 + n) * self.c, c2, 1)\n",
        "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3)), e=1.0) for _ in range(n))\n",
        "    def forward(self, x):\n",
        "        y = list(self.cv1(x).chunk(2, 1))\n",
        "        y.extend(m(y[-1]) for m in self.m)\n",
        "        return self.cv2(torch.cat(y, 1))\n",
        "\n",
        "class SPPF(nn.Module):\n",
        "    def __init__(self, c1, c2, k=5):\n",
        "        super().__init__()\n",
        "        c_ = c1 // 2\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
        "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        y1 = self.m(x)\n",
        "        y2 = self.m(y1)\n",
        "        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n",
        "\n",
        "class CSPDarknet(nn.Module):\n",
        "    def __init__(self, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        base_c = [64, 128, 256, 512, 1024]\n",
        "        base_d = [3, 6, 6, 3]\n",
        "        self.c = [int(x * width) for x in base_c]\n",
        "        self.d = [max(round(x * depth), 1) if x > 1 else x for x in base_d]\n",
        "        self.stem = Conv(3, self.c[0], 3, 2)\n",
        "        self.stage1 = nn.Sequential(Conv(self.c[0], self.c[1], 3, 2), C2f(self.c[1], self.c[1], n=self.d[0], shortcut=True))\n",
        "        self.stage2 = nn.Sequential(Conv(self.c[1], self.c[2], 3, 2), C2f(self.c[2], self.c[2], n=self.d[1], shortcut=True))\n",
        "        self.stage3 = nn.Sequential(Conv(self.c[2], self.c[3], 3, 2), C2f(self.c[3], self.c[3], n=self.d[2], shortcut=True))\n",
        "        self.stage4 = nn.Sequential(Conv(self.c[3], self.c[4], 3, 2), C2f(self.c[4], self.c[4], n=self.d[3], shortcut=True), SPPF(self.c[4], self.c[4], k=5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        c3 = self.stage2(x)\n",
        "        c4 = self.stage3(c3)\n",
        "        c5 = self.stage4(c4)\n",
        "        return c3, c4, c5\n",
        "\n",
        "class YOLOv8PAFPN(nn.Module):\n",
        "    def __init__(self, c3, c4, c5, out_ch=256, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce5 = Conv(c5, c4, 1, 1)\n",
        "        self.c2f_p4 = C2f(c4 + c4, c4, n=3, shortcut=False)\n",
        "        self.reduce4 = Conv(c4, c3, 1, 1)\n",
        "        self.c2f_p3 = C2f(c3 + c3, c3, n=3, shortcut=False)\n",
        "        self.down3 = Conv(c3, c3, 3, 2)\n",
        "        self.c2f_n4 = C2f(c3 + c4, c4, n=3, shortcut=False)\n",
        "        self.down4 = Conv(c4, c4, 3, 2)\n",
        "        self.c2f_n5 = C2f(c4 + c5, c5, n=3, shortcut=False)\n",
        "\n",
        "    def forward(self, c3, c4, c5):\n",
        "        p5 = c5\n",
        "        p4 = self.reduce5(p5)\n",
        "        p4_out = self.c2f_p4(torch.cat([self.up(p4), c4], dim=1))\n",
        "        p3 = self.reduce4(p4_out)\n",
        "        p3_out = self.c2f_p3(torch.cat([self.up(p3), c3], dim=1))\n",
        "        n3 = p3_out\n",
        "        n4_out = self.c2f_n4(torch.cat([self.down3(n3), p4_out], dim=1))\n",
        "        n5_out = self.c2f_n5(torch.cat([self.down4(n4_out), p5], dim=1))\n",
        "        return n3, n4_out, n5_out\n",
        "\n",
        "class Integral(nn.Module):\n",
        "    def __init__(self, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.reg_max = int(reg_max)\n",
        "        self.register_buffer(\"proj\", torch.arange(self.reg_max + 1, dtype=torch.float32), persistent=False)\n",
        "    def forward(self, logits):\n",
        "        return (logits.softmax(dim=-1) * self.proj).sum(dim=-1)\n",
        "\n",
        "class YoloV8LiteHead(nn.Module):\n",
        "    def __init__(self, in_channels_list, num_classes=80, hidden=256, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.reg_max = reg_max\n",
        "        self.integral = Integral(self.reg_max)\n",
        "        self.cls_towers = nn.ModuleList()\n",
        "        self.reg_towers = nn.ModuleList()\n",
        "        self.cls_preds = nn.ModuleList()\n",
        "        self.box_preds = nn.ModuleList()\n",
        "        \n",
        "        for in_ch in in_channels_list:\n",
        "            self.cls_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.reg_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.cls_preds.append(nn.Conv2d(hidden, num_classes, 1))\n",
        "            self.box_preds.append(nn.Conv2d(hidden, 4 * (self.reg_max + 1), 1))\n",
        "            \n",
        "        # Bias Init\n",
        "        p = 0.01\n",
        "        bias = -math.log((1 - p) / p)\n",
        "        for m in self.cls_preds:\n",
        "             nn.init.constant_(m.bias, bias)\n",
        "\n",
        "    def forward(self, features):\n",
        "        cls_outs = []\n",
        "        box_outs = []\n",
        "        for i, f in enumerate(features):\n",
        "            cls_outs.append(self.cls_preds[i](self.cls_towers[i](f)))\n",
        "            box_outs.append(self.box_preds[i](self.reg_towers[i](f)))\n",
        "        return cls_outs, box_outs\n",
        "\n",
        "class YoloModel(nn.Module):\n",
        "    def __init__(self, num_classes=80, backbone=\"yolov8_cspdarknet\", head_hidden=256, fpn_out=256):\n",
        "        super().__init__()\n",
        "        width = CFG.get(\"width\", 1.0)\n",
        "        depth = CFG.get(\"depth\", 1.0)\n",
        "        self.backbone = CSPDarknet(width=width, depth=depth)\n",
        "        base_c = [256, 512, 1024]\n",
        "        c3, c4, c5 = [int(x * width) for x in base_c]\n",
        "        self.neck = YOLOv8PAFPN(c3=c3, c4=c4, c5=c5, out_ch=fpn_out, width=width, depth=depth)\n",
        "        self.head = YoloV8LiteHead(in_channels_list=[c3, c4, c5], num_classes=num_classes, hidden=head_hidden, reg_max=CFG.get(\"reg_max\", 16))\n",
        "        self.strides = [8, 16, 32]\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        c3, c4, c5 = self.backbone(x)\n",
        "        p3, p4, p5 = self.neck(c3, c4, c5)\n",
        "        cls_outs, box_outs = self.head([p3, p4, p5])\n",
        "        head_out = {\"features\": [p3, p4, p5], \"cls\": cls_outs, \"box\": box_outs, \"strides\": self.strides}\n",
        "        \n",
        "        if self.training and targets is not None and hasattr(self, \"criterion\"):\n",
        "            losses, stats = self.criterion(head_out, targets)\n",
        "            return losses, stats\n",
        "        return head_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Utils, Loss & Training Helpers\n",
        "def make_grid(h, w, stride, device):\n",
        "    ys = torch.arange(h, device=device)\n",
        "    xs = torch.arange(w, device=device)\n",
        "    yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
        "    cx = (xx + 0.5) * stride\n",
        "    cy = (yy + 0.5) * stride\n",
        "    return cx.reshape(-1), cy.reshape(-1)\n",
        "\n",
        "def box_iou_xyxy_matrix(a, b):\n",
        "    if a.numel() == 0 or b.numel() == 0: return a.new_zeros((a.shape[0], b.shape[0]))\n",
        "    area_a = ((a[:, 2] - a[:, 0]).clamp(min=0) * (a[:, 3] - a[:, 1]).clamp(min=0))[:, None]\n",
        "    area_b = ((b[:, 2] - b[:, 0]).clamp(min=0) * (b[:, 3] - b[:, 1]).clamp(min=0))[None, :]\n",
        "    x1 = torch.maximum(a[:, None, 0], b[None, :, 0])\n",
        "    y1 = torch.maximum(a[:, None, 1], b[None, :, 1])\n",
        "    x2 = torch.minimum(a[:, None, 2], b[None, :, 2])\n",
        "    y2 = torch.minimum(a[:, None, 3], b[None, :, 3])\n",
        "    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n",
        "    return inter / (area_a + area_b - inter + 1e-6)\n",
        "\n",
        "def bbox_iou(box1, box2, eps=1e-7):\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
        "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n",
        "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n",
        "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
        "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
        "    union = w1 * h1 + w2 * h2 - inter + eps\n",
        "    return inter / union\n",
        "\n",
        "class DetectionLoss(nn.Module):\n",
        "    def __init__(self, num_classes, image_size, strides, lambda_box=7.5, lambda_cls=0.5, lambda_dfl=1.5, dfl_ch=17):\n",
        "        super().__init__()\n",
        "        self.nc = num_classes\n",
        "        self.imgsz = image_size\n",
        "        self.strides = strides\n",
        "        self.lambda_box = lambda_box\n",
        "        self.lambda_cls = lambda_cls\n",
        "        self.lambda_dfl = lambda_dfl\n",
        "        self.dfl_ch = dfl_ch\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, head_out, targets):\n",
        "        cls_outs = head_out[\"cls\"]\n",
        "        box_outs = head_out[\"box\"]\n",
        "        B = cls_outs[0].shape[0]\n",
        "        \n",
        "        pred_logits = torch.cat([x.permute(0, 2, 3, 1).reshape(B, -1, self.nc) for x in cls_outs], 1)\n",
        "        pred_dist = torch.cat([x.permute(0, 2, 3, 1).reshape(B, -1, 4 * self.dfl_ch) for x in box_outs], 1)\n",
        "        \n",
        "        targets_per_image, levels = self.build_targets(cls_outs, box_outs, targets)\n",
        "        \n",
        "        loss_cls = torch.tensor(0.0, device=cls_outs[0].device)\n",
        "        loss_box = torch.tensor(0.0, device=cls_outs[0].device)\n",
        "        num_pos_total = 0.0\n",
        "        total_anchors = pred_logits.shape[1]  # Total anchors per image for cls normalization\n",
        "        \n",
        "        for b in range(B):\n",
        "            t = targets_per_image[b]\n",
        "            pos_mask = t[\"pos_index\"]\n",
        "            num_pos = len(pos_mask)\n",
        "            num_pos_total += num_pos\n",
        "            \n",
        "            if num_pos > 0:\n",
        "                # Classification\n",
        "                t_cls = torch.zeros_like(pred_logits[b])\n",
        "                t_cls[pos_mask] = t[\"t_cls_soft\"].to(t_cls.dtype)\n",
        "                loss_cls += self.bce(pred_logits[b], t_cls).sum()\n",
        "                \n",
        "                # Box (IoU + DFL)\n",
        "                p_box_pos = pred_dist[b][pos_mask].view(-1, 4, self.dfl_ch)\n",
        "                \n",
        "                s_idx = torch.empty(num_pos, device=pred_dist.device, dtype=torch.float32)\n",
        "                for lev in levels:\n",
        "                    m = (pos_mask >= lev[\"start\"]) & (pos_mask < lev[\"end\"])\n",
        "                    if m.any(): s_idx[m] = float(lev[\"stride\"])\n",
        "                \n",
        "                t_bins = t[\"t_box_ltrb\"] / s_idx.unsqueeze(-1)\n",
        "                t_bins = t_bins.clamp(0, self.dfl_ch - 1.01)\n",
        "                tl = t_bins.long(); tr = tl + 1\n",
        "                wl = tr.float() - t_bins; wr = t_bins - tl.float()\n",
        "                \n",
        "                l_dfl = (F.cross_entropy(p_box_pos.view(-1, self.dfl_ch), tl.view(-1), reduction=\"none\").view(-1, 4) * wl +\n",
        "                         F.cross_entropy(p_box_pos.view(-1, self.dfl_ch), tr.view(-1), reduction=\"none\").view(-1, 4) * wr).mean(-1)\n",
        "                \n",
        "                p_ltrb = (p_box_pos.softmax(-1) * torch.arange(self.dfl_ch, device=p_box_pos.device).float()).sum(-1) * s_idx.unsqueeze(-1)\n",
        "                \n",
        "                anchors_cx, anchors_cy = [], []\n",
        "                for lev in levels:\n",
        "                    cx, cy = make_grid(lev[\"H\"], lev[\"W\"], lev[\"stride\"], pred_dist.device)\n",
        "                    anchors_cx.append(cx); anchors_cy.append(cy)\n",
        "                anchors_cx = torch.cat(anchors_cx)[pos_mask]\n",
        "                anchors_cy = torch.cat(anchors_cy)[pos_mask]\n",
        "                \n",
        "                p_xyxy = torch.stack([anchors_cx - p_ltrb[:, 0], anchors_cy - p_ltrb[:, 1],\n",
        "                                      anchors_cx + p_ltrb[:, 2], anchors_cy + p_ltrb[:, 3]], -1)\n",
        "                \n",
        "                iou = bbox_iou(p_xyxy, t[\"t_box_xyxy\"])\n",
        "                loss_box += ( (1.0 - iou) * self.lambda_box + l_dfl * self.lambda_dfl).sum()\n",
        "            else:\n",
        "                loss_cls += self.bce(pred_logits[b], torch.zeros_like(pred_logits[b])).sum()\n",
        "\n",
        "        # Normalize: cls by total anchors (stable), box by num positives\n",
        "        cls_norm = B * total_anchors  # Total anchor-class predictions\n",
        "        box_norm = max(num_pos_total, 1.0)\n",
        "        loss_cls_scaled = (loss_cls / cls_norm) * self.lambda_cls\n",
        "        loss_box_scaled = loss_box / box_norm\n",
        "        return {\"loss\": loss_cls_scaled + loss_box_scaled, \"loss_cls\": loss_cls_scaled, \"loss_box\": loss_box_scaled}, {\"num_pos\": num_pos_total}\n",
        "\n",
        "    def build_targets(self, cls_outs, box_outs, targets):\n",
        "        gt_classes = targets[\"labels\"]\n",
        "        gt_boxes = targets[\"boxes\"]\n",
        "        batch_idx = targets[\"batch_index\"]\n",
        "        B = cls_outs[0].shape[0]\n",
        "        gt_cls_list, gt_box_list = [], []\n",
        "        for i in range(B):\n",
        "            mask = batch_idx == i\n",
        "            gt_cls_list.append(gt_classes[mask]); gt_box_list.append(gt_boxes[mask])\n",
        "        return build_targets_task_aligned(cls_outs, box_outs, self.strides, gt_cls_list, gt_box_list, self.imgsz)\n",
        "\n",
        "def build_targets_task_aligned(cls_outs, box_outs, strides, gt_classes, gt_boxes_xyxy, image_size):\n",
        "    device = cls_outs[0].device\n",
        "    B, C = cls_outs[0].shape[0], cls_outs[0].shape[1]\n",
        "    levels, grids, start = [], [], 0\n",
        "    for (cl, s) in zip(cls_outs, strides):\n",
        "        _, _, H, W = cl.shape\n",
        "        levels.append({\"H\": H, \"W\": W, \"stride\": s, \"start\": start, \"end\": start + H * W})\n",
        "        cx, cy = make_grid(H, W, s, device)\n",
        "        grids.append((cx, cy)); start += H * W\n",
        "        \n",
        "    alpha, beta, topk, cr = CFG[\"tal_alpha\"], CFG[\"tal_beta\"], CFG[\"tal_topk\"], CFG[\"tal_center_radius\"]\n",
        "    per_image_targets = []\n",
        "    for b in range(B):\n",
        "        cls_flat = torch.cat([cl[b].permute(1, 2, 0).reshape(-1, C) for cl in cls_outs], 0)\n",
        "        gtc, gtb = gt_classes[b], gt_boxes_xyxy[b]\n",
        "        Ng = int(gtc.numel())\n",
        "        if Ng == 0:\n",
        "            per_image_targets.append({\"t_cls_soft\": torch.zeros(0, C, device=device), \"t_box_xyxy\": torch.zeros(0, 4, device=device), \"t_box_ltrb\": torch.zeros(0, 4, device=device), \"pos_index\": torch.zeros(0, dtype=torch.long, device=device)})\n",
        "            continue\n",
        "        \n",
        "        preds_xyxy = []\n",
        "        for (bx, lev, (cx, cy)) in zip(box_outs, levels, grids):\n",
        "            bl = bx[b].view(4, 17, -1).permute(2, 0, 1).softmax(-1)\n",
        "            dist = (bl * torch.arange(17, device=device)).sum(-1) * lev[\"stride\"]\n",
        "            preds_xyxy.append(torch.stack([cx - dist[:, 0], cy - dist[:, 1], cx + dist[:, 2], cy + dist[:, 3]], -1))\n",
        "        preds_xyxy = torch.cat(preds_xyxy, 0).clamp(0, image_size)\n",
        "\n",
        "        mask = torch.zeros(cls_flat.shape[0], Ng, dtype=torch.bool, device=device)\n",
        "        for lev, (cx, cy) in zip(levels, grids):\n",
        "            gt_centers = 0.5 * (gtb[:, :2] + gtb[:, 2:])\n",
        "            half = cr * lev[\"stride\"]\n",
        "            in_cr = (cx.view(-1, 1) >= gt_centers[:, 0] - half) & (cy.view(-1, 1) >= gt_centers[:, 1] - half) & \\\n",
        "                    (cx.view(-1, 1) <= gt_centers[:, 0] + half) & (cy.view(-1, 1) <= gt_centers[:, 1] + half)\n",
        "            mask[lev[\"start\"]:lev[\"end\"]] |= in_cr\n",
        "                \n",
        "        align = (cls_flat.sigmoid()[:, gtc].pow(alpha)) * (box_iou_xyxy_matrix(preds_xyxy, gtb).pow(beta))\n",
        "        align = torch.where(mask, align, torch.full_like(align, -1e-9))\n",
        "        \n",
        "        val, idx = torch.topk(align, min(topk, align.shape[0]), dim=0)\n",
        "        best_gt = torch.full((cls_flat.shape[0],), -1, dtype=torch.long, device=device)\n",
        "        best_score = torch.full((cls_flat.shape[0],), -1e-9, device=device)\n",
        "        for j in range(Ng):\n",
        "            better = val[:, j] > best_score[idx[:, j]]\n",
        "            best_gt[idx[better, j]] = j\n",
        "            best_score[idx[better, j]] = val[better, j]\n",
        "            \n",
        "        pos = best_gt >= 0; p_idx = torch.nonzero(pos).squeeze(1)\n",
        "        if p_idx.numel() == 0:\n",
        "            per_image_targets.append({\"t_cls_soft\": torch.zeros(0, C, device=device), \"t_box_xyxy\": torch.zeros(0, 4, device=device), \"t_box_ltrb\": torch.zeros(0, 4, device=device), \"pos_index\": p_idx})\n",
        "            continue\n",
        "        \n",
        "        gt_idx = best_gt[p_idx]; soft = torch.zeros(len(p_idx), C, device=device); soft[torch.arange(len(p_idx)), gtc[gt_idx]] = best_score[p_idx]\n",
        "        ltrb = torch.empty(len(p_idx), 4, device=device)\n",
        "        for i, lev in enumerate(levels):\n",
        "            m = (p_idx >= lev[\"start\"]) & (p_idx < lev[\"end\"])\n",
        "            if m.any():\n",
        "                ii = p_idx[m] - lev[\"start\"]\n",
        "                ltrb[m] = torch.stack([grids[i][0][ii] - gtb[gt_idx[m], 0], grids[i][1][ii] - gtb[gt_idx[m], 1], gtb[gt_idx[m], 2] - grids[i][0][ii], gtb[gt_idx[m], 3] - grids[i][1][ii]], -1)\n",
        "        per_image_targets.append({\"t_cls_soft\": soft, \"t_box_xyxy\": gtb[gt_idx], \"t_box_ltrb\": ltrb, \"pos_index\": p_idx})\n",
        "    return per_image_targets, levels\n",
        "\n",
        "def decode_outputs(head_out, strides, conf_thres=0.25, iou_thres=0.45, max_det=300, imgsz=640):\n",
        "    device = head_out[\"cls\"][0].device\n",
        "    B, C = head_out[\"cls\"][0].shape[0], head_out[\"cls\"][0].shape[1]\n",
        "    final_preds = []\n",
        "    for b in range(B):\n",
        "        boxes, scores, clss = [], [], []\n",
        "        for i, s in enumerate(strides):\n",
        "            cls = head_out[\"cls\"][i][b].permute(1, 2, 0).reshape(-1, C).sigmoid()\n",
        "            box = head_out[\"box\"][i][b].view(4, 17, -1).permute(2, 0, 1).softmax(-1)\n",
        "            dist = (box * torch.arange(17, device=device)).sum(-1) * s\n",
        "            cx, cy = make_grid(head_out[\"cls\"][i].shape[2], head_out[\"cls\"][i].shape[3], s, device)\n",
        "            \n",
        "            sc, cl = cls.max(1)\n",
        "            mask = sc > conf_thres\n",
        "            if mask.any():\n",
        "                bx = torch.stack([cx[mask] - dist[mask, 0], cy[mask] - dist[mask, 1], cx[mask] + dist[mask, 2], cy[mask] + dist[mask, 3]], -1)\n",
        "                boxes.append(bx); scores.append(sc[mask]); clss.append(cl[mask])\n",
        "        \n",
        "        if not boxes: final_preds.append(torch.zeros(0, 6, device=device)); continue\n",
        "        boxes, scores, clss = torch.cat(boxes), torch.cat(scores), torch.cat(clss)\n",
        "        \n",
        "        # Clamp boxes to image size\n",
        "        boxes.clamp_(0, imgsz)\n",
        "\n",
        "        keep = torch.ops.torchvision.nms(boxes, scores, iou_thres)\n",
        "        \n",
        "        # Limit max detections\n",
        "        keep = keep[:max_det]\n",
        "        \n",
        "        final_preds.append(torch.cat([boxes[keep], scores[keep, None], clss[keep, None].float()], 1))\n",
        "    return final_preds\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    mrec = np.concatenate(([0.], recall, [1.]))\n",
        "    mpre = np.concatenate(([1.], precision, [0.]))\n",
        "    for i in range(mpre.size - 1, 0, -1): mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "    return np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "\n",
        "def validate(model, loader, device, conf_thres=0.001, max_det=300):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    total_preds = 0\n",
        "    num_imgs = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            out = model(imgs)\n",
        "            preds = decode_outputs(out, [8, 16, 32], conf_thres=conf_thres, max_det=max_det, imgsz=CFG[\"imgsz\"])\n",
        "            for p in preds:\n",
        "                total_preds += len(p)\n",
        "                num_imgs += 1\n",
        "            if num_imgs > 100: break # Small sample for speed\n",
        "    \n",
        "    avg_preds = total_preds / max(1, num_imgs)\n",
        "    if was_training: model.train()\n",
        "    return avg_preds\n",
        "\n",
        "def visualize_inline(model, imgs, targets, step, device, classes):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(imgs[:2])\n",
        "        preds = decode_outputs(out, [8, 16, 32])\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    for i in range(min(len(imgs), 2)):\n",
        "        img = imgs[i].cpu().permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(img)\n",
        "        # Plot GT\n",
        "        mask = targets[\"batch_index\"] == i\n",
        "        for b in targets[\"boxes\"][mask].cpu().numpy():\n",
        "            axes[i].add_patch(Rectangle((b[0], b[1]), b[2]-b[0], b[3]-b[1], fill=False, color='green'))\n",
        "        # Plot Pred\n",
        "        for p in preds[i].cpu().numpy():\n",
        "            if p[4] > 0.3:\n",
        "                axes[i].add_patch(Rectangle((p[0], p[1]), p[2]-p[0], p[3]-p[1], fill=False, color='red'))\n",
        "                axes[i].text(p[0], p[1], f\"{classes[int(p[5])]}:{p[4]:.2f}\", color='white', backgroundcolor='red', fontsize=8)\n",
        "    axes[0].set_title(f\"Step {step} - Green:GT Red:Pred\")\n",
        "    plt.show()\n",
        "\n",
        "class ModelEMA:\n",
        "    def __init__(self, model, decay=0.9998, tau=2000):\n",
        "        self.module = copy.deepcopy(model).eval()\n",
        "        for p in self.module.parameters(): p.requires_grad_(False)\n",
        "        self.decay = decay\n",
        "        self.tau = tau\n",
        "        self.updates = 0\n",
        "    def update(self, model):\n",
        "        self.updates += 1\n",
        "        d = self.decay * (1 - math.exp(-self.updates / self.tau))\n",
        "        msd = model.state_dict(); esd = self.module.state_dict()\n",
        "        for k in esd.keys():\n",
        "            if esd[k].dtype.is_floating_point: esd[k].mul_(d).add_(msd[k].detach(), alpha=1.0 - d)\n",
        "            else: esd[k].copy_(msd[k])\n",
        "\n",
        "def save_checkpoint(epoch, model, ema, optimizer, scaler, scheduler, run_dir, step, filename=\"last.pt\"):\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch, \"global_step\": step,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"ema\": ema.module.state_dict(),\n",
        "        \"ema_updates\": ema.updates,\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "        \"scheduler\": scheduler.state_dict(),\n",
        "        \"cfg\": CFG\n",
        "    }\n",
        "    path = os.path.join(run_dir, filename)\n",
        "    tmp_path = path + \".tmp\"\n",
        "    torch.save(ckpt, tmp_path)\n",
        "    if os.path.exists(path):\n",
        "        os.remove(path)\n",
        "    os.rename(tmp_path, path)\n",
        "    print(f\"üì• Saved checkpoint: {path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Dataset (YOLOv8 Style)\n",
        "def augment_hsv(image, hgain=0.015, sgain=0.7, vgain=0.4):\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
        "    hsv[..., 0] *= (1 + (random.random() * 2 - 1) * hgain); hsv[..., 0] = np.clip(hsv[..., 0], 0, 179)\n",
        "    hsv[..., 1] *= (1 + (random.random() * 2 - 1) * sgain); hsv[..., 1] = np.clip(hsv[..., 1], 0, 255)\n",
        "    hsv[..., 2] *= (1 + (random.random() * 2 - 1) * vgain); hsv[..., 2] = np.clip(hsv[..., 2], 0, 255)\n",
        "    return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
        "\n",
        "def letterbox(img, new_shape=640, color=(114, 114, 114)):\n",
        "    shape = img.shape[:2]\n",
        "    r = min(new_shape / shape[0], new_shape / shape[1])\n",
        "    new_unpad = (int(round(shape[1] * r)), int(round(shape[0] * r)))\n",
        "    dw, dh = (new_shape - new_unpad[0]) / 2, (new_shape - new_unpad[1]) / 2\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "    return img, r, (left, top)\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, img_dir, lbl_dir, imgsz=640, augment=True):\n",
        "        self.img_dir, self.lbl_dir, self.imgsz, self.augment = img_dir, lbl_dir, imgsz, augment\n",
        "        self.img_files = sorted(glob.glob(os.path.join(img_dir, \"*.*\")))\n",
        "        self.lbl_files = [os.path.join(lbl_dir, Path(f).stem + \".txt\") for f in self.img_files]\n",
        "    def __len__(self): return len(self.img_files)\n",
        "    def __getitem__(self, i):\n",
        "        try:\n",
        "            img = cv2.imread(self.img_files[i])\n",
        "            if img is None: raise ValueError(\"Image decode failed\")\n",
        "            h0, w0 = img.shape[:2]\n",
        "            if self.augment: img = augment_hsv(img)\n",
        "            img, r, (padw, padh) = letterbox(img, self.imgsz)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Warning: Dataset failure on {self.img_files[i]}: {e}\")\n",
        "            img = np.full((self.imgsz, self.imgsz, 3), 114, dtype=np.uint8)\n",
        "            return torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0, {\"labels\": torch.zeros(0, dtype=torch.long), \"boxes\": torch.zeros((0, 4), dtype=torch.float32)}\n",
        "        \n",
        "        boxes = []\n",
        "        if os.path.exists(self.lbl_files[i]):\n",
        "            with open(self.lbl_files[i]) as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:  # Skip empty lines\n",
        "                        continue\n",
        "                    parts = line.split()\n",
        "                    if len(parts) == 5:\n",
        "                        try:\n",
        "                            c, x, y, w, h = map(float, parts)\n",
        "                            x1, y1 = (x - w/2) * w0 * r + padw, (y - h/2) * h0 * r + padh\n",
        "                            x2, y2 = (x + w/2) * w0 * r + padw, (y + h/2) * h0 * r + padh\n",
        "                            boxes.append([c, x1, y1, x2, y2])\n",
        "                        except ValueError:\n",
        "                            continue  # Skip malformed lines\n",
        "        \n",
        "        boxes = np.array(boxes) if boxes else np.zeros((0, 5))\n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
        "        return img, {\"labels\": torch.from_numpy(boxes[:, 0]).long(), \"boxes\": torch.from_numpy(boxes[:, 1:]).float()}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = zip(*batch)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    all_boxes, all_labels, all_idx = [], [], []\n",
        "    for i, t in enumerate(targets):\n",
        "        all_boxes.append(t[\"boxes\"])\n",
        "        all_labels.append(t[\"labels\"])\n",
        "        all_idx.append(torch.full((len(t[\"labels\"]),), i, dtype=torch.long))\n",
        "    return imgs, {\"boxes\": torch.cat(all_boxes, 0), \"labels\": torch.cat(all_labels, 0), \"batch_index\": torch.cat(all_idx, 0)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Dataset Preparation\n",
        "COCO_CLASSES = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "def ensure_dataset_ready(size=5000):\n",
        "    out_dir = CFG[\"data_root\"]\n",
        "    if os.path.exists(os.path.join(out_dir, \".complete\")):\n",
        "        print(f\"‚úÖ Dataset ready at {out_dir}\"); return\n",
        "    \n",
        "    print(f\"üì¶ Downloading COCO (size={size})...\")\n",
        "    if os.path.exists(out_dir): shutil.rmtree(out_dir)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    \n",
        "    for split in [\"train\", \"validation\"]:\n",
        "        ds = foz.load_zoo_dataset(\"coco-2017\", split=split, label_types=[\"detections\"], max_samples=size if split==\"train\" else 500, shuffle=True)\n",
        "        ds.export(export_dir=out_dir, dataset_type=fo.types.YOLOv5Dataset, label_field=\"ground_truth\", split=\"train\" if split==\"train\" else \"val\", classes=COCO_CLASSES)\n",
        "        fo.delete_dataset(ds.name)\n",
        "        \n",
        "    # Verify label integrity\n",
        "    print(\"üîé Verifying label indices (0-79)...\")\n",
        "    train_lbl = os.path.join(out_dir, \"labels/train\")\n",
        "    if os.path.exists(train_lbl):\n",
        "        bad_files = 0\n",
        "        for lf in glob.glob(os.path.join(train_lbl, \"*.txt\"))[:500]:\n",
        "             with open(lf) as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        c = int(float(line.split()[0]))\n",
        "                        if c < 0 or c >= 80: print(f\"‚ùå Bad class {c} in {lf}\"); bad_files += 1\n",
        "                    except: pass\n",
        "        if bad_files == 0: print(\"‚úÖ Label indices look correct.\")\n",
        "\n",
        "    with open(os.path.join(out_dir, \".complete\"), \"w\") as f: f.write(\"done\")\n",
        "    print(f\"‚úÖ Export complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Main Training Loop\n",
        "set_seed(CFG[\"seed\"])\n",
        "ensure_dataset_ready(size=NUM_SAMPLES)\n",
        "\n",
        "train_loader = DataLoader(YoloDataset(os.path.join(CFG[\"data_root\"], \"images/train\"), os.path.join(CFG[\"data_root\"], \"labels/train\")), batch_size=CFG[\"batch_size\"], shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
        "val_loader = DataLoader(YoloDataset(os.path.join(CFG[\"data_root\"], \"images/val\"), os.path.join(CFG[\"data_root\"], \"labels/val\"), augment=False), batch_size=CFG[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Calculate total training steps for proper LR scheduling\n",
        "steps_per_epoch = len(train_loader)\n",
        "total_training_steps = steps_per_epoch * CFG[\"epochs\"]\n",
        "warmup_steps = min(1000, total_training_steps // 10)  # 10% warmup, max 1000 steps\n",
        "print(f\"Steps per epoch: {steps_per_epoch}, Total steps: {total_training_steps}, Warmup steps: {warmup_steps}\")\n",
        "\n",
        "model = YoloModel(num_classes=CFG[\"num_classes\"]).to(device)\n",
        "\n",
        "if CFG.get(\"pretrained\"):\n",
        "    pt = CFG[\"pretrained\"]\n",
        "    if os.path.exists(pt):\n",
        "        print(f\"üì¶ Loading weights from {pt}...\")\n",
        "        st = torch.load(pt, map_location=device)\n",
        "        model.load_state_dict(st[\"model\"] if \"model\" in st else st, strict=False)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Pretrained {pt} not found.\")\n",
        "\n",
        "model.criterion = DetectionLoss(num_classes=80, image_size=640, strides=[8,16,32]).to(device)\n",
        "ema = ModelEMA(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n",
        "scaler = GradScaler(\"cuda\", enabled=CFG[\"amp\"])\n",
        "\n",
        "# Cosine scheduler with warmup (LambdaLR)\n",
        "def get_lr(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / warmup_steps\n",
        "    else:\n",
        "        progress = (step - warmup_steps) / max(1, total_training_steps - warmup_steps)\n",
        "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "        min_lr_ratio = CFG.get(\"min_lr_ratio\", 0.05)\n",
        "        return min_lr_ratio + (1 - min_lr_ratio) * cosine\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr)\n",
        "\n",
        "start_epoch, global_step = 0, 0\n",
        "\n",
        "# Skip checkpoint loading in QUICK_TEST mode (always start fresh)\n",
        "if not QUICK_TEST:\n",
        "    ckpt_path = os.path.join(RUN_DIR, \"last.pt\")\n",
        "    if os.path.exists(ckpt_path):\n",
        "        ckpt = torch.load(ckpt_path, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"]); ema.module.load_state_dict(ckpt[\"ema\"]); ema.updates = ckpt[\"ema_updates\"]\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"]); scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "        if \"scheduler\" in ckpt: scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "        start_epoch, global_step = ckpt[\"epoch\"], ckpt[\"global_step\"]\n",
        "        print(f\"Resumed from epoch {start_epoch}, step {global_step}, LR={scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "accumulate = CFG.get(\"accumulate\", 1)\n",
        "log_file = os.path.join(RUN_DIR, \"train_log.csv\")\n",
        "if not os.path.exists(log_file):\n",
        "    with open(log_file, \"w\") as f: f.write(\"epoch,step,loss,lr,avg_pos,val_avg_preds\n",
        "\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, CFG[\"epochs\"]):\n",
        "        if time.time() - start_time > CFG[\"time_limit\"]:\n",
        "            print(f\"‚è∞ Time limit reached ({CFG['time_limit']}s). Stopping training.\")\n",
        "            break\n",
        "            \n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CFG['epochs']}\")\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        total_loss = 0.0\n",
        "        total_num_pos = 0.0\n",
        "        \n",
        "        for batch_idx, (imgs, targets) in enumerate(pbar):\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            for k, v in targets.items(): targets[k] = v.to(device, non_blocking=True)\n",
        "            \n",
        "            with torch.amp.autocast(\"cuda\", enabled=CFG[\"amp\"]):\n",
        "                losses, stats = model(imgs, targets)\n",
        "                loss = losses[\"loss\"] / accumulate\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            if (batch_idx + 1) % accumulate == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.get(\"grad_clip_norm\", 10.0))\n",
        "                \n",
        "                scale_before = scaler.get_scale()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                \n",
        "                # Only step scheduler/EMA/global_step if the optimizer actually stepped (not skipped by AMP)\n",
        "                if scaler.get_scale() >= scale_before:\n",
        "                    scheduler.step()\n",
        "                    ema.update(model)\n",
        "                    global_step += 1\n",
        "                optimizer.zero_grad()\n",
        "            \n",
        "            total_loss += losses[\"loss\"].item()\n",
        "            total_num_pos += stats[\"num_pos\"]\n",
        "            \n",
        "            avg_loss = total_loss * accumulate / (batch_idx + 1)\n",
        "            avg_pos = total_num_pos / (batch_idx + 1)\n",
        "            \n",
        "            if batch_idx % 10 == 0:\n",
        "                pbar.set_postfix({\"loss\": f\"{avg_loss:.3f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\", \"avg_pos\": f\"{avg_pos:.1f}\"})\n",
        "            \n",
        "            if global_step % 200 == 0 and global_step > 0: \n",
        "                visualize_inline(ema.module, imgs, targets, global_step, device, COCO_CLASSES)\n",
        "        \n",
        "        # Validation\n",
        "        val_preds = validate(ema.module, val_loader, device, conf_thres=0.001)\n",
        "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Avg Pos: {avg_pos:.1f} | Val Avg Preds/Img: {val_preds:.2f} (EMA)\")\n",
        "        \n",
        "        with open(log_file, \"a\") as f:\n",
        "             f.write(f\"{epoch+1},{global_step},{avg_loss:.4f},{scheduler.get_last_lr()[0]:.2e},{avg_pos:.1f},{val_preds:.2f}\n",
        "\")\n",
        "\n",
        "        # Skip checkpoint saving in QUICK_TEST mode\n",
        "        if not QUICK_TEST:\n",
        "            save_checkpoint(epoch + 1, model, ema, optimizer, scaler, scheduler, RUN_DIR, global_step)\n",
        "        \n",
        "except KeyboardInterrupt: \n",
        "    print(\"üõë Interrupted by User! Saving checkpoint...\")\n",
        "    if not QUICK_TEST:\n",
        "        save_checkpoint(epoch, model, ema, optimizer, scaler, scheduler, RUN_DIR, global_step)\n",
        "\n",
        "print(\"Training complete!\" if not QUICK_TEST else \"Quick test complete!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
