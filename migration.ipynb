{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fiftyone\n",
            "  Downloading fiftyone-1.11.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from fiftyone) (24.1.0)\n",
            "Collecting argcomplete (from fiftyone)\n",
            "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting async_lru>=2 (from fiftyone)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from fiftyone) (4.13.5)\n",
            "Collecting boto3 (from fiftyone)\n",
            "  Downloading boto3-1.42.19-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from fiftyone) (6.2.4)\n",
            "Collecting dacite<2,>=1.6.0 (from fiftyone)\n",
            "  Downloading dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from fiftyone) (0.3.8)\n",
            "Collecting Deprecated (from fiftyone)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting ftfy (from fiftyone)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from fiftyone) (4.14.0)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone)\n",
            "  Downloading hypercorn-0.18.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.12/dist-packages (from fiftyone) (3.1.6)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.12/dist-packages (from fiftyone) (1.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from fiftyone) (3.10.0)\n",
            "Collecting mongoengine~=0.29.1 (from fiftyone)\n",
            "  Downloading mongoengine-0.29.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting motor~=3.6.0 (from fiftyone)\n",
            "  Downloading motor-3.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fiftyone) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fiftyone) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from fiftyone) (2.2.2)\n",
            "Requirement already satisfied: Pillow!=11.2.*,>=6.2 in /usr/local/lib/python3.12/dist-packages (from fiftyone) (11.3.0)\n",
            "Collecting plotly>=6.1.1 (from fiftyone)\n",
            "  Downloading plotly-6.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pprintpp (from fiftyone)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from fiftyone) (5.9.5)\n",
            "Collecting pymongo~=4.9.2 (from fiftyone)\n",
            "  Downloading pymongo-4.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from fiftyone) (2025.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from fiftyone) (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from fiftyone) (2025.11.3)\n",
            "Collecting retrying (from fiftyone)\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: rtree in /usr/local/lib/python3.12/dist-packages (from fiftyone) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from fiftyone) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from fiftyone) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from fiftyone) (1.16.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from fiftyone) (75.2.0)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: starlette>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from fiftyone) (0.50.0)\n",
            "Collecting strawberry-graphql>=0.262.4 (from fiftyone)\n",
            "  Downloading strawberry_graphql-0.288.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fiftyone) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fiftyone) (4.67.1)\n",
            "Collecting xmltodict (from fiftyone)\n",
            "  Downloading xmltodict-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting pydash (from fiftyone)\n",
            "  Downloading pydash-8.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting fiftyone-brain<0.22,>=0.21.4 (from fiftyone)\n",
            "  Downloading fiftyone_brain-0.21.4-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting fiftyone-db<2.0,>=0.4 (from fiftyone)\n",
            "  Downloading fiftyone_db-1.4.1.tar.gz (8.6 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting voxel51-eta<0.16,>=0.15.1 (from fiftyone)\n",
            "  Downloading voxel51_eta-0.15.1-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from fiftyone) (4.12.0.88)\n",
            "Requirement already satisfied: h11 in /usr/local/lib/python3.12/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.16.0)\n",
            "Requirement already satisfied: h2>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from hypercorn>=0.13.2->fiftyone) (4.3.0)\n",
            "Collecting priority (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading priority-2.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3->fiftyone) (3.0.3)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from plotly>=6.1.1->fiftyone) (2.13.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo~=4.9.2->fiftyone)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette>=0.24.0->fiftyone) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from starlette>=0.24.0->fiftyone) (4.15.0)\n",
            "Collecting cross-web>=0.4.0 (from strawberry-graphql>=0.262.4->fiftyone)\n",
            "  Downloading cross_web-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql>=0.262.4->fiftyone)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from strawberry-graphql>=0.262.4->fiftyone) (2.9.0.post0)\n",
            "Requirement already satisfied: httpx>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.28.1)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting paramiko<4,>=3 (from voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading py7zr-1.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting rarfile (from voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone) (2.32.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone) (5.3.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from voxel51-eta<0.16,>=0.15.1->fiftyone) (2.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->fiftyone) (2.8)\n",
            "Collecting botocore<1.43.0,>=1.42.19 (from boto3->fiftyone)\n",
            "  Downloading botocore-1.42.19-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3->fiftyone)\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from Deprecated->fiftyone) (2.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->fiftyone) (0.2.14)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch->fiftyone) (3.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fiftyone) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fiftyone) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fiftyone) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fiftyone) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->fiftyone) (3.2.5)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->fiftyone) (2025.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->fiftyone) (3.6.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->fiftyone) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->fiftyone) (2025.12.12)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->fiftyone) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->fiftyone) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->fiftyone) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone) (3.11)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2>=4.3.0->hypercorn>=0.13.2->fiftyone) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2>=4.3.0->hypercorn>=0.13.2->fiftyone) (4.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.0.9)\n",
            "Collecting bcrypt>=3.2 (from paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.12/dist-packages (from paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone) (43.0.3)\n",
            "Collecting pynacl>=1.5 (from paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading pynacl-1.6.1-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->strawberry-graphql>=0.262.4->fiftyone) (1.17.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines->voxel51-eta<0.16,>=0.15.1->fiftyone) (25.4.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone) (3.23.0)\n",
            "Requirement already satisfied: brotli>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone) (1.2.0)\n",
            "Collecting backports.zstd>=1.0.0 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading backports_zstd-1.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting pyppmd>=1.3.1 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading pyppmd-1.3.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting pybcj>=1.0.6 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading pybcj-1.0.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64>=1.0.4 (from py7zr->voxel51-eta<0.16,>=0.15.1->fiftyone)\n",
            "  Downloading inflate64-1.0.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->voxel51-eta<0.16,>=0.15.1->fiftyone) (3.4.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.3->paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko<4,>=3->voxel51-eta<0.16,>=0.15.1->fiftyone) (2.23)\n",
            "Downloading fiftyone-1.11.0-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading fiftyone_brain-0.21.4-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypercorn-0.18.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mongoengine-0.29.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading motor-3.6.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly-6.5.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading strawberry_graphql-0.288.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.1/313.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Downloading voxel51_eta-0.15.1-py2.py3-none-any.whl (934 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.3/934.3 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.42.19-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading xmltodict-1.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading botocore-1.42.19-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading cross_web-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Downloading py7zr-1.1.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Downloading backports_zstd-1.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.2/494.2 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading pybcj-1.0.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynacl-1.6.1-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.3.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: fiftyone-db\n",
            "  Building wheel for fiftyone-db (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fiftyone-db: filename=fiftyone_db-1.4.1-py3-none-manylinux1_x86_64.whl size=50655254 sha256=4b997e1cab0ee944af9d9b3da913326bf14f023805985118586a8cf1ba18d1cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/06/06/0eb7530b99c4934bf36390769cff3f0052fd04f50f390d5c8b\n",
            "Successfully built fiftyone-db\n",
            "Installing collected packages: texttable, sseclient-py, pprintpp, xmltodict, wsproto, retrying, rarfile, pyppmd, pydash, pybcj, priority, plotly, multivolumefile, jsonlines, jmespath, inflate64, graphql-core, ftfy, fiftyone-db, dnspython, Deprecated, dacite, cross-web, bcrypt, backports.zstd, async_lru, argcomplete, strawberry-graphql, pynacl, pymongo, py7zr, hypercorn, botocore, universal-analytics-python3, sse-starlette, s3transfer, paramiko, motor, mongoengine, fiftyone-brain, voxel51-eta, boto3, fiftyone\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "  Attempting uninstall: sse-starlette\n",
            "    Found existing installation: sse-starlette 3.0.4\n",
            "    Uninstalling sse-starlette-3.0.4:\n",
            "      Successfully uninstalled sse-starlette-3.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mcp 1.24.0 requires sse-starlette>=1.6.1, but you have sse-starlette 0.10.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.3.1 argcomplete-3.6.3 async_lru-2.0.5 backports.zstd-1.3.0 bcrypt-5.0.0 boto3-1.42.19 botocore-1.42.19 cross-web-0.4.0 dacite-1.9.2 dnspython-2.8.0 fiftyone-1.11.0 fiftyone-brain-0.21.4 fiftyone-db-1.4.1 ftfy-6.3.1 graphql-core-3.2.7 hypercorn-0.18.0 inflate64-1.0.4 jmespath-1.0.1 jsonlines-4.0.0 mongoengine-0.29.1 motor-3.6.1 multivolumefile-0.2.3 paramiko-3.5.1 plotly-6.5.0 pprintpp-0.4.0 priority-2.0.0 py7zr-1.1.0 pybcj-1.0.7 pydash-8.0.5 pymongo-4.9.2 pynacl-1.6.1 pyppmd-1.3.1 rarfile-4.2 retrying-1.4.2 s3transfer-0.16.0 sse-starlette-0.10.3 sseclient-py-1.8.0 strawberry-graphql-0.288.1 texttable-1.7.0 universal-analytics-python3-1.1.1 voxel51-eta-0.15.1 wsproto-1.3.2 xmltodict-1.0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/glob2/fnmatch.py:141: SyntaxWarning: invalid escape sequence '\\Z'\n",
            "  return '(?ms)' + res + '\\Z'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "✅ Notebook Updated: Version 3.0 (Fixed Collate & Criterion)\n"
          ]
        }
      ],
      "source": [
        "%pip install fiftyone\n",
        "# 1. Imports & Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import copy\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "# Set Seed\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"✅ Notebook Updated: Version 3.0 (Fixed Collate & Criterion)\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run Directory: /content/yolo-lab/runs/20251231_161801_yolov8_local_batch\n"
          ]
        }
      ],
      "source": [
        "# 2. Configuration\n",
        "# --- USER SETTINGS ---\n",
        "QUICK_TEST = False  # Set to True for a fast smoke test (1 batch, 10 images)\n",
        "BATCH_SIZE = 100 if QUICK_TEST else 2000  # Images per \"roll\"\n",
        "NUM_BATCHES = 1 if QUICK_TEST else 10    # How many times to roll\n",
        "EPOCHS_PER_BATCH = 1 if QUICK_TEST else 2 # Epochs to train on each batch\n",
        "\n",
        "BASE_DIR = os.path.abspath(\"yolo-lab\")\n",
        "DIRS = {\n",
        "    \"datasets\": os.path.join(BASE_DIR, \"datasets\"),\n",
        "    \"runs\": os.path.join(BASE_DIR, \"runs\"),\n",
        "    \"configs\": os.path.join(BASE_DIR, \"configs\"),\n",
        "}\n",
        "for d in DIRS.values():\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "EXP_NAME = \"yolov8_local_batch\"\n",
        "RUN_NAME = f\"{timestamp}_{EXP_NAME}\"\n",
        "RUN_DIR = os.path.join(DIRS[\"runs\"], RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "# Model & Training Config\n",
        "CFG = {\n",
        "    \"exp_name\": EXP_NAME,\n",
        "    \"run_name\": RUN_NAME,\n",
        "    \"seed\": 42,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch_size\": 8 if QUICK_TEST else 16,\n",
        "    \"num_classes\": 80,\n",
        "    \n",
        "    # Model\n",
        "    \"width\": 1.0,\n",
        "    \"depth\": 1.0,\n",
        "    \"reg_max\": 16,\n",
        "    \"head_hidden\": 256,\n",
        "    \"backbone\": \"yolov8_cspdarknet\",\n",
        "    \n",
        "    # Optimizer\n",
        "    \"optimizer\": \"adamw\",\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"cosine_schedule\": True,\n",
        "    \"epochs\": EPOCHS_PER_BATCH, # Per batch\n",
        "    \"amp\": True,\n",
        "    \"grad_clip_norm\": 10.0,\n",
        "    \"ema_decay\": 0.9998,\n",
        "    \n",
        "    # Loss\n",
        "    \"tal_alpha\": 1.0,\n",
        "    \"tal_beta\": 6.0,\n",
        "    \"tal_topk\": 10,\n",
        "    \"tal_center_radius\": 2.5,\n",
        "    \"loss_weights\": {\"box\": 7.5, \"cls\": 0.5, \"dfl\": 1.5}, # Adjusted for v8\n",
        "    \n",
        "    # Augmentation\n",
        "    \"letterbox_pad\": 114,\n",
        "    \"hflip_p\": 0.5,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.7,\n",
        "    \"hsv_v\": 0.4,\n",
        "    \n",
        "    # Paths (Dynamic per batch)\n",
        "    \"data_root\": os.path.join(DIRS[\"datasets\"], \"current_batch\"),\n",
        "    \"train_img_dir\": \"images/train\",\n",
        "    \"train_lbl_dir\": \"labels/train\",\n",
        "    \"val_img_dir\": \"images/val\",\n",
        "    \"val_lbl_dir\": \"labels/val\",\n",
        "}\n",
        "\n",
        "print(\"Run Directory:\", RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Model Architecture\n",
        "def autopad(k, p=None, d=1):\n",
        "    if d > 1:\n",
        "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]\n",
        "    if p is None:\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n",
        "    return p\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    default_act = nn.SiLU()\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
        "        super().__init__()\n",
        "        c_ = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
        "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "class C2f(nn.Module):\n",
        "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
        "        super().__init__()\n",
        "        self.c = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
        "        self.cv2 = Conv((2 + n) * self.c, c2, 1)\n",
        "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3)), e=1.0) for _ in range(n))\n",
        "    def forward(self, x):\n",
        "        y = list(self.cv1(x).chunk(2, 1))\n",
        "        y.extend(m(y[-1]) for m in self.m)\n",
        "        return self.cv2(torch.cat(y, 1))\n",
        "\n",
        "class SPPF(nn.Module):\n",
        "    def __init__(self, c1, c2, k=5):\n",
        "        super().__init__()\n",
        "        c_ = c1 // 2\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
        "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        y1 = self.m(x)\n",
        "        y2 = self.m(y1)\n",
        "        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n",
        "\n",
        "class CSPDarknet(nn.Module):\n",
        "    def __init__(self, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        base_c = [64, 128, 256, 512, 1024]\n",
        "        base_d = [3, 6, 6, 3]\n",
        "        self.c = [int(x * width) for x in base_c]\n",
        "        self.d = [max(round(x * depth), 1) if x > 1 else x for x in base_d]\n",
        "        self.stem = Conv(3, self.c[0], 3, 2)\n",
        "        self.stage1 = nn.Sequential(Conv(self.c[0], self.c[1], 3, 2), C2f(self.c[1], self.c[1], n=self.d[0], shortcut=True))\n",
        "        self.stage2 = nn.Sequential(Conv(self.c[1], self.c[2], 3, 2), C2f(self.c[2], self.c[2], n=self.d[1], shortcut=True))\n",
        "        self.stage3 = nn.Sequential(Conv(self.c[2], self.c[3], 3, 2), C2f(self.c[3], self.c[3], n=self.d[2], shortcut=True))\n",
        "        self.stage4 = nn.Sequential(Conv(self.c[3], self.c[4], 3, 2), C2f(self.c[4], self.c[4], n=self.d[3], shortcut=True), SPPF(self.c[4], self.c[4], k=5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        c3 = self.stage2(x)\n",
        "        c4 = self.stage3(c3)\n",
        "        c5 = self.stage4(c4)\n",
        "        return c3, c4, c5\n",
        "\n",
        "class YOLOv8PAFPN(nn.Module):\n",
        "    def __init__(self, c3, c4, c5, out_ch=256, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce5 = Conv(c5, c4, 1, 1)\n",
        "        self.c2f_p4 = C2f(c4 + c4, c4, n=3, shortcut=False)\n",
        "        self.reduce4 = Conv(c4, c3, 1, 1)\n",
        "        self.c2f_p3 = C2f(c3 + c3, c3, n=3, shortcut=False)\n",
        "        self.down3 = Conv(c3, c3, 3, 2)\n",
        "        self.c2f_n4 = C2f(c3 + c4, c4, n=3, shortcut=False)\n",
        "        self.down4 = Conv(c4, c4, 3, 2)\n",
        "        self.c2f_n5 = C2f(c4 + c5, c5, n=3, shortcut=False)\n",
        "\n",
        "    def forward(self, c3, c4, c5):\n",
        "        p5 = c5\n",
        "        p4 = self.reduce5(p5)\n",
        "        p4_out = self.c2f_p4(torch.cat([self.up(p4), c4], dim=1))\n",
        "        p3 = self.reduce4(p4_out)\n",
        "        p3_out = self.c2f_p3(torch.cat([self.up(p3), c3], dim=1))\n",
        "        n3 = p3_out\n",
        "        n4_out = self.c2f_n4(torch.cat([self.down3(n3), p4_out], dim=1))\n",
        "        n5_out = self.c2f_n5(torch.cat([self.down4(n4_out), p5], dim=1))\n",
        "        return n3, n4_out, n5_out\n",
        "\n",
        "class Integral(nn.Module):\n",
        "    def __init__(self, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.reg_max = int(reg_max)\n",
        "        self.register_buffer(\"proj\", torch.arange(self.reg_max + 1, dtype=torch.float32), persistent=False)\n",
        "    def forward(self, logits):\n",
        "        return (logits.softmax(dim=-1) * self.proj).sum(dim=-1)\n",
        "\n",
        "class YoloV8LiteHead(nn.Module):\n",
        "    def __init__(self, in_channels_list, num_classes=80, hidden=256, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.reg_max = reg_max\n",
        "        self.integral = Integral(self.reg_max)\n",
        "        self.cls_towers = nn.ModuleList()\n",
        "        self.reg_towers = nn.ModuleList()\n",
        "        self.cls_preds = nn.ModuleList()\n",
        "        self.box_preds = nn.ModuleList()\n",
        "        \n",
        "        for in_ch in in_channels_list:\n",
        "            self.cls_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.reg_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.cls_preds.append(nn.Conv2d(hidden, num_classes, 1))\n",
        "            self.box_preds.append(nn.Conv2d(hidden, 4 * (self.reg_max + 1), 1))\n",
        "            \n",
        "        # Bias Init (Fix: High loss magnitude)\n",
        "        import math\n",
        "        p = 0.01\n",
        "        bias = -math.log((1 - p) / p)\n",
        "        for m in self.cls_preds:\n",
        "             nn.init.constant_(m.bias, bias)\n",
        "\n",
        "    def forward(self, features):\n",
        "        cls_outs = []\n",
        "        box_outs = []\n",
        "        for i, f in enumerate(features):\n",
        "            cls_outs.append(self.cls_preds[i](self.cls_towers[i](f)))\n",
        "            box_outs.append(self.box_preds[i](self.reg_towers[i](f)))\n",
        "        return cls_outs, box_outs\n",
        "\n",
        "class YoloModel(nn.Module):\n",
        "    def __init__(self, num_classes=80, backbone=\"yolov8_cspdarknet\", head_hidden=256, fpn_out=256):\n",
        "        super().__init__()\n",
        "        width = CFG.get(\"width\", 1.0)\n",
        "        depth = CFG.get(\"depth\", 1.0)\n",
        "        self.backbone = CSPDarknet(width=width, depth=depth)\n",
        "        base_c = [256, 512, 1024]\n",
        "        c3, c4, c5 = [int(x * width) for x in base_c]\n",
        "        self.neck = YOLOv8PAFPN(c3=c3, c4=c4, c5=c5, out_ch=fpn_out, width=width, depth=depth)\n",
        "        self.head = YoloV8LiteHead(in_channels_list=[c3, c4, c5], num_classes=num_classes, hidden=head_hidden, reg_max=CFG.get(\"reg_max\", 16))\n",
        "        self.strides = [8, 16, 32]\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        c3, c4, c5 = self.backbone(x)\n",
        "        p3, p4, p5 = self.neck(c3, c4, c5)\n",
        "        cls_outs, box_outs = self.head([p3, p4, p5])\n",
        "        head_out = {\"features\": [p3, p4, p5], \"cls\": cls_outs, \"box\": box_outs, \"strides\": self.strides}\n",
        "        \n",
        "        if self.training and targets is not None and hasattr(self, \"criterion\"):\n",
        "            losses, stats = self.criterion(head_out, targets)\n",
        "            return losses, stats\n",
        "        return head_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Utils & Loss\n",
        "def make_grid(h, w, stride, device):\n",
        "    ys = torch.arange(h, device=device)\n",
        "    xs = torch.arange(w, device=device)\n",
        "    yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
        "    cx = (xx + 0.5) * stride\n",
        "    cy = (yy + 0.5) * stride\n",
        "    return cx.reshape(-1), cy.reshape(-1)\n",
        "\n",
        "def box_iou_xyxy_matrix(a, b):\n",
        "    if a.numel() == 0 or b.numel() == 0: return a.new_zeros((a.shape[0], b.shape[0]))\n",
        "    area_a = ((a[:, 2] - a[:, 0]).clamp(min=0) * (a[:, 3] - a[:, 1]).clamp(min=0))[:, None]\n",
        "    area_b = ((b[:, 2] - b[:, 0]).clamp(min=0) * (b[:, 3] - b[:, 1]).clamp(min=0))[None, :]\n",
        "    x1 = torch.maximum(a[:, None, 0], b[None, :, 0])\n",
        "    y1 = torch.maximum(a[:, None, 1], b[None, :, 1])\n",
        "    x2 = torch.minimum(a[:, None, 2], b[None, :, 2])\n",
        "    y2 = torch.minimum(a[:, None, 3], b[None, :, 3])\n",
        "    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n",
        "    return inter / (area_a + area_b - inter + 1e-6)\n",
        "\n",
        "def bbox_iou(box1, box2, eps=1e-7):\n",
        "    # box1: [N, 4], box2: [N, 4]\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
        "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n",
        "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n",
        "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
        "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
        "    union = w1 * h1 + w2 * h2 - inter + eps\n",
        "    return inter / union\n",
        "\n",
        "class DetectionLoss(nn.Module):\n",
        "    def __init__(self, num_classes, image_size, strides, lambda_box=7.5, lambda_cls=0.5, lambda_dfl=1.5, dfl_ch=17):\n",
        "        super().__init__()\n",
        "        self.nc = num_classes\n",
        "        self.imgsz = image_size\n",
        "        self.strides = strides\n",
        "        self.lambda_box = lambda_box\n",
        "        self.lambda_cls = lambda_cls\n",
        "        self.lambda_dfl = lambda_dfl\n",
        "        self.dfl_ch = dfl_ch\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, head_out, targets):\n",
        "        cls_outs = head_out[\"cls\"]\n",
        "        box_outs = head_out[\"box\"]\n",
        "        \n",
        "        # Vectorized Prediction (Fix: Efficiency)\n",
        "        B = cls_outs[0].shape[0]\n",
        "        \n",
        "        pred_logits = [x.permute(0, 2, 3, 1).reshape(B, -1, self.nc) for x in cls_outs]\n",
        "        preds_cls = torch.cat(pred_logits, dim=1) # [B, N_anchors, nc]\n",
        "        \n",
        "        pred_dist = [x.permute(0, 2, 3, 1).reshape(B, -1, 4 * self.dfl_ch) for x in box_outs]\n",
        "        preds_box = torch.cat(pred_dist, dim=1) # [B, N_anchors, 4*dfl_ch]\n",
        "        \n",
        "        targets_per_image, levels = self.build_targets(cls_outs, box_outs, targets)\n",
        "        \n",
        "        loss_cls = torch.tensor(0.0, device=cls_outs[0].device)\n",
        "        loss_box = torch.tensor(0.0, device=cls_outs[0].device)\n",
        "        num_pos_total = 0.0\n",
        "        \n",
        "        for b in range(B):\n",
        "            t = targets_per_image[b]\n",
        "            pos_mask = t[\"pos_index\"]\n",
        "            num_pos = len(pos_mask)\n",
        "            num_pos_total += num_pos\n",
        "            \n",
        "            # Classification Loss\n",
        "            pred_cls_b = preds_cls[b] # [N_anchors, nc]\n",
        "            t_cls = torch.zeros_like(pred_cls_b)\n",
        "            if num_pos > 0:\n",
        "                t_cls[pos_mask] = t[\"t_cls_soft\"].to(t_cls.dtype)\n",
        "            \n",
        "            l_cls = self.bce(pred_cls_b, t_cls).sum()\n",
        "            loss_cls += l_cls\n",
        "            \n",
        "            # Box Loss (IoU + DFL)\n",
        "            if num_pos > 0:\n",
        "                # Extract only pos anchors from pre-reshaped tensor\n",
        "                pred_box_dist_pos = preds_box[b][pos_mask] # [num_pos, 4*dfl_ch]\n",
        "                pred_box_pos = pred_box_dist_pos.view(-1, 4, self.dfl_ch)\n",
        "                \n",
        "                # Get strides for pos anchors\n",
        "                anchor_strides = torch.empty(num_pos, device=preds_box.device, dtype=torch.float32)\n",
        "                for level in levels:\n",
        "                     start, end, s = level[\"start\"], level[\"end\"], level[\"stride\"]\n",
        "                     in_level = (pos_mask >= start) & (pos_mask < end)\n",
        "                     if in_level.any():\n",
        "                         anchor_strides[in_level] = float(s)\n",
        "                \n",
        "                # DFL Targets (Option A: Current t_box_ltrb is in pixels)\n",
        "                t_box_ltrb_pixels = t[\"t_box_ltrb\"]\n",
        "                t_box_ltrb_bins = t_box_ltrb_pixels / anchor_strides.unsqueeze(-1)\n",
        "                t_box_ltrb_bins = t_box_ltrb_bins.clamp(max=self.dfl_ch - 1.01)\n",
        "\n",
        "                tl = t_box_ltrb_bins.long()\n",
        "                tr = tl + 1\n",
        "                wl = tr.float() - t_box_ltrb_bins\n",
        "                wr = t_box_ltrb_bins - tl.float()\n",
        "                \n",
        "                l_dfl = (F.cross_entropy(pred_box_pos.view(-1, self.dfl_ch), tl.view(-1), reduction=\"none\").view(-1, 4) * wl +\n",
        "                         F.cross_entropy(pred_box_pos.view(-1, self.dfl_ch), tr.view(-1), reduction=\"none\").view(-1, 4) * wr).mean(-1)\n",
        "                \n",
        "                # IoU Decode\n",
        "                anchors_cx, anchors_cy = [], []\n",
        "                for level_i, level in enumerate(levels):\n",
        "                    cx, cy = make_grid(level[\"H\"], level[\"W\"], level[\"stride\"], preds_box.device)\n",
        "                    anchors_cx.append(cx)\n",
        "                    anchors_cy.append(cy)\n",
        "                anchors_cx = torch.cat(anchors_cx)[pos_mask]\n",
        "                anchors_cy = torch.cat(anchors_cy)[pos_mask]\n",
        "                \n",
        "                pred_ltrb_bins = (pred_box_pos.softmax(dim=-1) * torch.arange(self.dfl_ch, device=pred_box_pos.device).float()).sum(dim=-1)\n",
        "                pred_ltrb_pixels = pred_ltrb_bins * anchor_strides.unsqueeze(-1)\n",
        "                \n",
        "                pred_x1 = anchors_cx - pred_ltrb_pixels[:, 0]\n",
        "                pred_y1 = anchors_cy - pred_ltrb_pixels[:, 1]\n",
        "                pred_x2 = anchors_cx + pred_ltrb_pixels[:, 2]\n",
        "                pred_y2 = anchors_cy + pred_ltrb_pixels[:, 3]\n",
        "                pred_xyxy = torch.stack([pred_x1, pred_y1, pred_x2, pred_y2], dim=-1)\n",
        "                \n",
        "                iou = bbox_iou(pred_xyxy, t[\"t_box_xyxy\"])\n",
        "                l_iou = 1.0 - iou\n",
        "                \n",
        "                loss_box += (l_iou * self.lambda_box + l_dfl * self.lambda_dfl).sum()\n",
        "\n",
        "        # Normalize\n",
        "        norm = max(num_pos_total, 1.0)\n",
        "        loss_cls = (loss_cls * self.lambda_cls) / norm\n",
        "        loss_box = loss_box / norm\n",
        "        \n",
        "        return {\"loss\": loss_cls + loss_box, \"loss_cls\": loss_cls, \"loss_box\": loss_box}, {\"num_pos\": num_pos_total}\n",
        "\n",
        "    def build_targets(self, cls_outs, box_outs, targets):\n",
        "        gt_classes = targets[\"labels\"]\n",
        "        gt_boxes = targets[\"boxes\"]\n",
        "        batch_idx = targets[\"batch_index\"]\n",
        "        \n",
        "        B = cls_outs[0].shape[0]\n",
        "        gt_cls_list = []\n",
        "        gt_box_list = []\n",
        "        for i in range(B):\n",
        "            mask = batch_idx == i\n",
        "            gt_cls_list.append(gt_classes[mask])\n",
        "            gt_box_list.append(gt_boxes[mask])\n",
        "            \n",
        "        return build_targets_task_aligned(cls_outs, box_outs, self.strides, gt_cls_list, gt_box_list, self.imgsz)\n",
        "\n",
        "def build_targets_task_aligned(cls_outs, box_outs, strides, gt_classes, gt_boxes_xyxy, image_size):\n",
        "    device = cls_outs[0].device\n",
        "    B = cls_outs[0].shape[0]\n",
        "    C = cls_outs[0].shape[1]\n",
        "    \n",
        "    levels = []\n",
        "    start = 0\n",
        "    grids = []\n",
        "    \n",
        "    for (cl, s) in zip(cls_outs, strides):\n",
        "        _, _, H, W = cl.shape\n",
        "        levels.append({\"H\": H, \"W\": W, \"stride\": s, \"start\": start, \"end\": start + H * W})\n",
        "        cx, cy = make_grid(H, W, s, device)\n",
        "        grids.append((cx, cy))\n",
        "        start += H * W\n",
        "        \n",
        "    tal_alpha = float(CFG.get(\"tal_alpha\", 1.0))\n",
        "    tal_beta = float(CFG.get(\"tal_beta\", 6.0))\n",
        "    tal_topk = int(CFG.get(\"tal_topk\", 10))\n",
        "    tal_cr = float(CFG.get(\"tal_center_radius\", 2.5))\n",
        "    \n",
        "    per_image_targets = []\n",
        "    for b in range(B):\n",
        "        cls_per_image = [cl[b].permute(1, 2, 0).reshape(-1, C) for cl in cls_outs]\n",
        "        cls_flat = torch.cat(cls_per_image, dim=0)\n",
        "        N_total = cls_flat.shape[0]\n",
        "        \n",
        "        gtc = gt_classes[b]\n",
        "        gtb = gt_boxes_xyxy[b]\n",
        "        Ng = int(gtc.numel())\n",
        "        \n",
        "        if Ng == 0:\n",
        "            per_image_targets.append({\n",
        "                \"t_cls_soft\": torch.zeros(0, C, device=device),\n",
        "                \"t_box_xyxy\": torch.zeros(0, 4, device=device),\n",
        "                \"t_box_ltrb\": torch.zeros(0, 4, device=device),\n",
        "                \"pos_index\": torch.zeros(0, dtype=torch.long, device=device),\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        pred_xyxy_levels = []\n",
        "        for (bx, level, (cx, cy)) in zip(box_outs, levels, grids):\n",
        "            H, W, s = level[\"H\"], level[\"W\"], level[\"stride\"]\n",
        "            bl = bx[b]\n",
        "            M1 = bl.shape[0] // 4\n",
        "            bl = bl.view(4, M1, H, W).permute(2, 3, 0, 1).reshape(H * W, 4, M1)\n",
        "            probs = bl.softmax(dim=-1)\n",
        "            proj = torch.arange(M1, device=device, dtype=bl.dtype)\n",
        "            dists = (probs * proj).sum(dim=-1) * float(s)\n",
        "            \n",
        "            x1 = cx - dists[:, 0]\n",
        "            y1 = cy - dists[:, 1]\n",
        "            x2 = cx + dists[:, 2]\n",
        "            y2 = cy + dists[:, 3]\n",
        "            pred_xyxy_levels.append(torch.stack([x1, y1, x2, y2], dim=-1).clamp_(0, image_size))\n",
        "            \n",
        "        pred_xyxy = torch.cat(pred_xyxy_levels, dim=0)\n",
        "\n",
        "        candidate_mask = torch.zeros(N_total, Ng, dtype=torch.bool, device=device)\n",
        "        for level, (cx, cy) in enumerate(grids):\n",
        "            start, end, s = levels[level][\"start\"], levels[level][\"end\"], levels[level][\"stride\"]\n",
        "            Nl = end - start\n",
        "            cxv, cyv = cx.view(Nl, 1), cy.view(Nl, 1)\n",
        "            \n",
        "            if tal_cr > 0:\n",
        "                gt_centers = 0.5 * (gtb[:, :2] + gtb[:, 2:])\n",
        "                half = tal_cr * s\n",
        "                in_center = (cxv >= gt_centers[:, 0] - half) & (cyv >= gt_centers[:, 1] - half) & \\\n",
        "                            (cxv <= gt_centers[:, 0] + half) & (cyv <= gt_centers[:, 1] + half)\n",
        "                candidate_mask[start:end] |= in_center\n",
        "            else:\n",
        "                in_box = (cxv >= gtb[:, 0]) & (cyv >= gtb[:, 1]) & (cxv <= gtb[:, 2]) & (cyv <= gtb[:, 3])\n",
        "                candidate_mask[start:end] |= in_box\n",
        "                \n",
        "        cls_sigmoid = cls_flat.sigmoid()\n",
        "        cls_gt_scores = cls_sigmoid[:, gtc]\n",
        "        iou_matrix = box_iou_xyxy_matrix(pred_xyxy, gtb)\n",
        "        align = (cls_gt_scores.clamp(min=1e-9).pow(tal_alpha)) * (iou_matrix.clamp(min=1e-9).pow(tal_beta))\n",
        "        align = torch.where(candidate_mask, align, torch.full_like(align, -1e-9))\n",
        "        \n",
        "        k = min(tal_topk, align.shape[0])\n",
        "        topk_scores, topk_index = torch.topk(align, k, dim=0)\n",
        "        \n",
        "        best_gt_per_pred = torch.full((N_total,), -1, dtype=torch.long, device=device)\n",
        "        best_score_per_pred = torch.full((N_total,), -1e-9, dtype=align.dtype, device=device)\n",
        "        \n",
        "        for j in range(Ng):\n",
        "            idx_j = topk_index[:, j]\n",
        "            score_j = topk_scores[:, j]\n",
        "            better = score_j > best_score_per_pred[idx_j]\n",
        "            best_gt_per_pred[idx_j[better]] = j\n",
        "            best_score_per_pred[idx_j[better]] = score_j[better]\n",
        "            \n",
        "        pos_mask = best_gt_per_pred >= 0\n",
        "        pos_index = torch.nonzero(pos_mask, as_tuple=False).squeeze(1)\n",
        "        \n",
        "        if pos_index.numel() == 0:\n",
        "            per_image_targets.append({\n",
        "                \"t_cls_soft\": torch.zeros(0, C, device=device),\n",
        "                \"t_box_xyxy\": torch.zeros(0, 4, device=device),\n",
        "                \"t_box_ltrb\": torch.zeros(0, 4, device=device),\n",
        "                \"pos_index\": pos_index,\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        gt_index = best_gt_per_pred[pos_index]\n",
        "        scores = best_score_per_pred[pos_index].clamp(min=0.0)\n",
        "        t_cls_soft = torch.zeros(len(pos_index), C, device=device)\n",
        "        t_cls_soft[torch.arange(len(pos_index)), gtc[gt_index]] = scores\n",
        "        t_box_xyxy = gtb[gt_index]\n",
        "        \n",
        "        t_box_ltrb = torch.empty(len(pos_index), 4, device=device)\n",
        "        for level_i, level in enumerate(levels):\n",
        "            start, end, s = level[\"start\"], level[\"end\"], level[\"stride\"]\n",
        "            cx, cy = grids[level_i]\n",
        "            in_level = (pos_index >= start) & (pos_index < end)\n",
        "            if in_level.any():\n",
        "                idx_l = pos_index[in_level] - start\n",
        "                ct = torch.stack((cx[idx_l], cy[idx_l]), dim=-1)\n",
        "                gs = gtb[gt_index[in_level]]\n",
        "                t_box_ltrb[in_level] = torch.stack((ct[:,0]-gs[:,0], ct[:,1]-gs[:,1], gs[:,2]-ct[:,0], gs[:,3]-ct[:,1]), dim=-1).clamp(min=0, max=float(image_size))\n",
        "\n",
        "        per_image_targets.append({\n",
        "            \"t_cls_soft\": t_cls_soft,\n",
        "            \"t_box_xyxy\": t_box_xyxy,\n",
        "            \"t_box_ltrb\": t_box_ltrb,\n",
        "            \"pos_index\": pos_index,\n",
        "        })\n",
        "    \n",
        "    return per_image_targets, levels\n",
        "\n",
        "# --- Visualization Utils --- (Fix 4 included implicitly)\n",
        "def decode_outputs(head_out, strides, conf_thres=0.25, iou_thres=0.45, max_det=300):\n",
        "    # head_out: dict with 'cls' and 'box'\n",
        "    device = head_out[\"cls\"][0].device\n",
        "    cls_outs = head_out[\"cls\"]\n",
        "    box_outs = head_out[\"box\"]\n",
        "    B = cls_outs[0].shape[0]\n",
        "    \n",
        "    preds = []\n",
        "    \n",
        "    # Generate grids\n",
        "    grids = []\n",
        "    for (cl, s) in zip(cls_outs, strides):\n",
        "        _, _, H, W = cl.shape\n",
        "        cx, cy = make_grid(H, W, s, device)\n",
        "        grids.append((cx, cy, s))\n",
        "        \n",
        "    for b in range(B):\n",
        "        # Collect all predictions for this image\n",
        "        batch_boxes = []\n",
        "        batch_scores = []\n",
        "        batch_classes = []\n",
        "        \n",
        "        for i, (bx, (cx, cy, s)) in enumerate(zip(box_outs, grids)):\n",
        "            # Box decoding\n",
        "            bl = bx[b] # [4*reg_max, H, W]\n",
        "            C_box = bl.shape[0]\n",
        "            reg_max = C_box // 4\n",
        "            H, W = bl.shape[1], bl.shape[2]\n",
        "            \n",
        "            bl = bl.view(4, reg_max, H*W).permute(2, 0, 1) # [HW, 4, reg_max]\n",
        "            probs = bl.softmax(dim=-1)\n",
        "            proj = torch.arange(reg_max, device=device, dtype=bl.dtype)\n",
        "            ltrb = (probs * proj).sum(dim=-1) * s # [HW, 4]\n",
        "            \n",
        "            x1 = cx - ltrb[:, 0]\n",
        "            y1 = cy - ltrb[:, 1]\n",
        "            x2 = cx + ltrb[:, 2]\n",
        "            y2 = cy + ltrb[:, 3]\n",
        "            xyxy = torch.stack([x1, y1, x2, y2], dim=-1)\n",
        "            \n",
        "            # Class scores\n",
        "            cl = cls_outs[i][b].permute(1, 2, 0).reshape(-1, cls_outs[i].shape[1]) # [HW, nc]\n",
        "            scores = cl.sigmoid()\n",
        "            \n",
        "            # Filter by conf\n",
        "            max_scores, max_classes = scores.max(dim=1)\n",
        "            mask = max_scores > conf_thres\n",
        "            \n",
        "            if mask.any():\n",
        "                batch_boxes.append(xyxy[mask])\n",
        "                batch_scores.append(max_scores[mask])\n",
        "                batch_classes.append(max_classes[mask])\n",
        "                \n",
        "        if not batch_boxes:\n",
        "            preds.append(torch.zeros(0, 6, device=device))\n",
        "            continue\n",
        "            \n",
        "        batch_boxes = torch.cat(batch_boxes, dim=0)\n",
        "        batch_scores = torch.cat(batch_scores, dim=0)\n",
        "        batch_classes = torch.cat(batch_classes, dim=0)\n",
        "        \n",
        "        # NMS\n",
        "        keep = torch.ops.torchvision.nms(batch_boxes, batch_scores, iou_thres)\n",
        "        keep = keep[:max_det]\n",
        "        \n",
        "        # [x1, y1, x2, y2, conf, cls]\n",
        "        out = torch.cat([batch_boxes[keep], batch_scores[keep, None], batch_classes[keep, None].float()], dim=1)\n",
        "        preds.append(out)\n",
        "        \n",
        "    return preds\n",
        "\n",
        "def visualize_batch_results(model, loader, device, run_dir, batch_idx, num_samples=3):\n",
        "    model.eval()\n",
        "    images, targets = next(iter(loader))\n",
        "    images = images.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        head_out = model(images)\n",
        "        preds = decode_outputs(head_out, model.strides)\n",
        "        \n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    if num_samples == 1: axes = [axes]\n",
        "    \n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        img = np.ascontiguousarray(img) * 255\n",
        "        img = img.astype(np.uint8)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        \n",
        "        # GT\n",
        "        gt_mask = targets[\"batch_index\"] == i\n",
        "        gt_boxes = targets[\"boxes\"][gt_mask]\n",
        "        for box in gt_boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.tolist())\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2) # Green\n",
        "            \n",
        "        # Pred\n",
        "        pred = preds[i]\n",
        "        for p in pred:\n",
        "            x1, y1, x2, y2, conf, cls = p.tolist()\n",
        "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2) # Red\n",
        "            cv2.putText(img, f\"{conf:.2f}\", (int(x1), int(y1)-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "            \n",
        "        if num_samples > 1:\n",
        "            axes[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "            axes[i].axis(\"off\")\n",
        "            axes[i].set_title(f\"Sample {i}\")\n",
        "        else:\n",
        "            axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "            axes[0].axis(\"off\")\n",
        "            axes[0].set_title(f\"Sample {i}\")\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(run_dir, f\"val_batch_{batch_idx}.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    print(f\"🖼️ Visualization saved to {save_path}\")\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Dataset & Dataloader\n",
        "def augment_hsv(img, hgain=0.015, sgain=0.7, vgain=0.4):\n",
        "    if hgain or sgain or vgain:\n",
        "        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
        "        hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_RGB2HSV))\n",
        "        dtype = img.dtype  # uint8\n",
        "\n",
        "        x = np.arange(0, 256, dtype=r.dtype)\n",
        "        lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
        "        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
        "        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
        "\n",
        "        im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
        "        cv2.cvtColor(im_hsv, cv2.COLOR_HSV2RGB, dst=img)\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, imgsz=640, augment=True, pad_value=114):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.imgsz = imgsz\n",
        "        self.augment = augment\n",
        "        self.pad_value = pad_value\n",
        "        \n",
        "        # Support multiple extensions\n",
        "        self.image_paths = []\n",
        "        for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"):\n",
        "            self.image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        \n",
        "        self.label_paths = [os.path.join(label_dir, Path(p).stem + \".txt\") for p in self.image_paths]\n",
        "        \n",
        "        if len(self.image_paths) == 0:\n",
        "            print(f\"⚠️ WARNING: No images found in {image_dir}\")\n",
        "            print(f\"   Did the export work? Check {os.path.dirname(image_dir)}\")\n",
        "        else:\n",
        "            print(f\"✅ Loaded {len(self.image_paths)} images from {image_dir}\")\n",
        "        \n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Load/Convert\n",
        "        img = cv2.imread(self.image_paths[index])\n",
        "        if img is None: # Safety check\n",
        "             print(f\"Error reading {self.image_paths[index]}\")\n",
        "             return torch.zeros(3, self.imgsz, self.imgsz), {}\n",
        "\n",
        "        # Fix: Convert BGR (OpenCV) to RGB (Model Expectation)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        h, w = img.shape[:2]\n",
        "        \n",
        "        # Read labels\n",
        "        lbl_path = self.label_paths[index]\n",
        "        boxes = []\n",
        "        cls = []\n",
        "        if os.path.exists(lbl_path):\n",
        "            with open(lbl_path) as f:\n",
        "                for line in f:\n",
        "                    parts = list(map(float, line.strip().split()))\n",
        "                    if len(parts) == 5:\n",
        "                        cls.append(int(parts[0]))\n",
        "                        # YOLO xywh to xyxy\n",
        "                        cx, cy, bw, bh = parts[1:]\n",
        "                        x1 = (cx - bw/2) * w\n",
        "                        y1 = (cy - bh/2) * h\n",
        "                        x2 = (cx + bw/2) * w\n",
        "                        y2 = (cy + bh/2) * h\n",
        "                        boxes.append([x1, y1, x2, y2])\n",
        "        \n",
        "        boxes = np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4), dtype=np.float32)\n",
        "        cls = np.array(cls, dtype=np.int64) if cls else np.zeros((0,), dtype=np.int64)\n",
        "        \n",
        "        # --- Augmentation (HSV + Flip) ---\n",
        "        if self.augment:\n",
        "             # HSV\n",
        "             augment_hsv(img, hgain=CFG[\"hsv_h\"], sgain=CFG[\"hsv_s\"], vgain=CFG[\"hsv_v\"])\n",
        "             \n",
        "             # Flip Left-Right\n",
        "             if random.random() < CFG.get(\"hflip_p\", 0.5):\n",
        "                 img = cv2.flip(img, 1)\n",
        "                 if len(boxes):\n",
        "                     # x1 -> w - x2, x2 -> w - x1\n",
        "                     boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
        "                     \n",
        "        # Letterbox (Simplified)\n",
        "        r = min(self.imgsz / h, self.imgsz / w)\n",
        "        nw, nh = int(w * r), int(h * r)\n",
        "        img = cv2.resize(img, (nw, nh))\n",
        "        \n",
        "        # Pad\n",
        "        pad_w = self.imgsz - nw\n",
        "        pad_h = self.imgsz - nh\n",
        "        img = cv2.copyMakeBorder(img, pad_h//2, pad_h-pad_h//2, pad_w//2, pad_w-pad_w//2, cv2.BORDER_CONSTANT, value=(114,114,114))\n",
        "        \n",
        "        # Adjust boxes\n",
        "        if len(boxes):\n",
        "            boxes[:, [0, 2]] = boxes[:, [0, 2]] * r + pad_w//2\n",
        "            boxes[:, [1, 3]] = boxes[:, [1, 3]] * r + pad_h//2\n",
        "            \n",
        "        # Helper: clamp boxes\n",
        "        if len(boxes):\n",
        "             boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, self.imgsz)\n",
        "             boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, self.imgsz)\n",
        "\n",
        "        # To Tensor\n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
        "        boxes = torch.from_numpy(boxes)\n",
        "        cls = torch.from_numpy(cls)\n",
        "        \n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": cls,\n",
        "            \"image_id\": Path(self.image_paths[index]).stem,\n",
        "            \"orig_size\": (h, w),\n",
        "            \"scale\": r,\n",
        "            \"pad\": (pad_w//2, pad_h//2)\n",
        "        }\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = list(zip(*batch))\n",
        "    # Remove failed loads (zeros)\n",
        "    valid = [i for i, (img, _) in enumerate(images) if img.shape[0] == 3]\n",
        "    if len(valid) < len(images):\n",
        "        images = [images[i] for i in valid]\n",
        "        targets = [targets[i] for i in valid]\n",
        "    \n",
        "    if not images:\n",
        "        return torch.tensor([]), {}\n",
        "        \n",
        "    images = torch.stack(images, dim=0)\n",
        "    \n",
        "    all_boxes = []\n",
        "    all_labels = []\n",
        "    all_bidx = []\n",
        "    image_ids = []\n",
        "    scales = []\n",
        "    pads = []\n",
        "    orig_sizes = []\n",
        "    \n",
        "    for i, t in enumerate(targets):\n",
        "        n = t[\"boxes\"].shape[0]\n",
        "        if n:\n",
        "            all_boxes.append(t[\"boxes\"])\n",
        "            all_labels.append(t[\"labels\"])\n",
        "            all_bidx.append(torch.full((n,), i, dtype=torch.long))\n",
        "            \n",
        "        image_ids.append(t[\"image_id\"])\n",
        "        scales.append(t[\"scale\"])\n",
        "        pads.append(t[\"pad\"])\n",
        "        orig_sizes.append(t[\"orig_size\"])\n",
        "        \n",
        "    if len(all_boxes):\n",
        "        boxes = torch.cat(all_boxes, 0)\n",
        "        labels = torch.cat(all_labels, 0)\n",
        "        bidx = torch.cat(all_bidx, 0)\n",
        "    else:\n",
        "        boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "        labels = torch.zeros((0,), dtype=torch.long)\n",
        "        bidx = torch.zeros((0,), dtype=torch.long)\n",
        "        \n",
        "    return images, {\n",
        "        \"boxes\": boxes,\n",
        "        \"labels\": labels,\n",
        "        \"batch_index\": bidx,\n",
        "        \"image_id\": image_ids,\n",
        "        \"scale\": scales,\n",
        "        \"pad\": pads,\n",
        "        \"orig_size\": orig_sizes\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Batch Management Logic\n",
        "COCO_CLASSES = [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "    'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "def prepare_batch(batch_idx, size=2000):\n",
        "    print(f\"\\n📦 Preparing Batch {batch_idx} (Size: {size})...\")\n",
        "    \n",
        "    train_name = f\"batch_{batch_idx}_train\"\n",
        "    val_name = f\"batch_{batch_idx}_val\"\n",
        "    \n",
        "    # 1. Safe Cleanup of Old Batches\n",
        "    if fo.list_datasets():\n",
        "        for name in fo.list_datasets():\n",
        "            if name.startswith(\"batch_\") and name != train_name and name != val_name:\n",
        "                print(f\"🧹 Deleting old dataset: {name}\")\n",
        "                fo.delete_dataset(name)\n",
        "    \n",
        "    for name in [train_name, val_name]:\n",
        "        if name in fo.list_datasets():\n",
        "            print(f\"♻️  Deleting existing {name} to ensure fresh start\")\n",
        "            fo.delete_dataset(name)\n",
        "\n",
        "    # 2. Download/Load from Zoo (Explicit Splits)\n",
        "    # Train\n",
        "    train_ds = foz.load_zoo_dataset(\n",
        "        \"coco-2017\",\n",
        "        split=\"train\", \n",
        "        label_types=[\"detections\"],\n",
        "        max_samples=size,\n",
        "        shuffle=True,\n",
        "        seed=batch_idx * 999, \n",
        "        dataset_name=train_name,\n",
        "        drop_existing=True \n",
        "    )\n",
        "\n",
        "    # Validation (Fixed small size for smoke tests)\n",
        "    val_ds = foz.load_zoo_dataset(\n",
        "        \"coco-2017\",\n",
        "        split=\"validation\", \n",
        "        label_types=[\"detections\"],\n",
        "        max_samples=100 if size < 500 else 500,\n",
        "        shuffle=True,\n",
        "        seed=batch_idx * 999, \n",
        "        dataset_name=val_name,\n",
        "        drop_existing=True \n",
        "    )\n",
        "    \n",
        "    # 3. Export to YOLO format (Explicit Directory Mapping)\n",
        "    out_dir = CFG[\"data_root\"]\n",
        "    if os.path.exists(out_dir):\n",
        "        shutil.rmtree(out_dir) \n",
        "        \n",
        "    # Export Train\n",
        "    train_ds.export(\n",
        "        export_dir=out_dir,\n",
        "        dataset_type=fo.types.YOLOv5Dataset,\n",
        "        label_field=\"ground_truth\",\n",
        "        split=\"train\",\n",
        "        classes=COCO_CLASSES\n",
        "    )\n",
        "    \n",
        "    # Export Val\n",
        "    val_ds.export(\n",
        "        export_dir=out_dir,\n",
        "        dataset_type=fo.types.YOLOv5Dataset,\n",
        "        label_field=\"ground_truth\",\n",
        "        split=\"val\",\n",
        "        classes=COCO_CLASSES\n",
        "    )\n",
        "    \n",
        "    print(f\"✅ Batch {batch_idx} exported to {out_dir}\")\n",
        "    return train_ds # Return main ds object for cleanup\n",
        "\n",
        "def cleanup_batch(dataset):\n",
        "    # Only need to clean up FiftyOne datasets, file delete is already done if needed or at start\n",
        "    # But actually we return a FiftyOne dataset object. We should delete both names.\n",
        "    print(\"🧹 Cleaning up batch...\")\n",
        "    # dataset.delete() # might only be train\n",
        "    if fo.list_datasets():\n",
        "         for name in fo.list_datasets():\n",
        "             if name.startswith(\"batch_\"):\n",
        "                 fo.delete_dataset(name)\n",
        "                 \n",
        "    if os.path.exists(CFG[\"data_root\"]):\n",
        "        shutil.rmtree(CFG[\"data_root\"])\n",
        "    print(\"✨ Cleanup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== STARTING BATCH 1/10 === \n",
            "\n",
            "📦 Preparing Batch 0 (Size: 2000)...\n",
            "Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |██████|    1.9Gb/1.9Gb [4.8s elapsed, 0s remaining, 455.4Mb/s]       \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    1.9Gb/1.9Gb [4.8s elapsed, 0s remaining, 455.4Mb/s]       \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        }
      ],
      "source": [
        "# 7. Main Execution Loop\n",
        "state_file = os.path.join(DIRS[\"runs\"], \"checkpoint_state.json\")\n",
        "last_ckpt_path = os.path.join(RUN_DIR, \"last.pt\")\n",
        "\n",
        "# --- Initialization (ONCE) ---\n",
        "model = YoloModel(num_classes=CFG[\"num_classes\"]).to(device)\n",
        "criterion = DetectionLoss(\n",
        "    num_classes=CFG[\"num_classes\"],\n",
        "    image_size=CFG[\"imgsz\"],\n",
        "    strides=[8, 16, 32],\n",
        "    lambda_box=CFG[\"loss_weights\"][\"box\"],\n",
        "    lambda_cls=CFG[\"loss_weights\"][\"cls\"],\n",
        "    lambda_dfl=CFG[\"loss_weights\"].get(\"dfl\", 1.5)\n",
        ")\n",
        "model.criterion = criterion\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"])\n",
        "scaler = GradScaler(enabled=CFG[\"amp\"])\n",
        "\n",
        "# --- Resume Logic ---\n",
        "start_batch = 0\n",
        "start_epoch = 0\n",
        "global_step = 0\n",
        "\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    print(f\"🔄 Resuming from {last_ckpt_path}\")\n",
        "    ckpt = torch.load(last_ckpt_path)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "    \n",
        "    # Restore state\n",
        "    start_batch = ckpt.get(\"batch_idx\", 0)\n",
        "    start_epoch = ckpt.get(\"epoch\", 0)\n",
        "    global_step = ckpt.get(\"global_step\", 0)\n",
        "    \n",
        "    # If we finished the last batch completely, move to next\n",
        "    if ckpt.get(\"batch_complete\", False):\n",
        "        start_batch += 1\n",
        "        start_epoch = 0\n",
        "        \n",
        "    print(f\"   -> Batch: {start_batch}, Epoch: {start_epoch}, Step: {global_step}\")\n",
        "\n",
        "# --- Helper: Save Checkpoint ---\n",
        "def save_checkpoint(path, batch_idx, epoch, step, complete=False):\n",
        "    torch.save({\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "        \"batch_idx\": batch_idx,\n",
        "        \"epoch\": epoch,\n",
        "        \"global_step\": step,\n",
        "        \"batch_complete\": complete\n",
        "    }, path)\n",
        "    # Also save state json for easy reading\n",
        "    with open(state_file, \"w\") as f:\n",
        "        json.dump({\"batch_idx\": batch_idx, \"epoch\": epoch, \"global_step\": step}, f)\n",
        "\n",
        "# --- Main Loop ---\n",
        "try:\n",
        "    for b_idx in range(start_batch, NUM_BATCHES):\n",
        "        print(f\"\\n=== STARTING BATCH {b_idx + 1}/{NUM_BATCHES} === \")\n",
        "        \n",
        "        # 1. Prepare Data\n",
        "        ds = prepare_batch(b_idx, size=BATCH_SIZE)\n",
        "        \n",
        "        # Detect images folder\n",
        "        if os.path.isdir(os.path.join(CFG[\"data_root\"], \"images/train\")):\n",
        "            train_img_path = os.path.join(CFG[\"data_root\"], \"images/train\")\n",
        "            train_lbl_path = os.path.join(CFG[\"data_root\"], \"labels/train\")\n",
        "        elif os.path.isdir(os.path.join(CFG[\"data_root\"], \"images/val\")):\n",
        "            print(\"⚠️ Using images/val as training source\")\n",
        "            train_img_path = os.path.join(CFG[\"data_root\"], \"images/val\")\n",
        "            train_lbl_path = os.path.join(CFG[\"data_root\"], \"labels/val\")\n",
        "        else:\n",
        "            # Maybe root?\n",
        "            train_img_path = CFG[\"data_root\"]\n",
        "            train_lbl_path = CFG[\"data_root\"] \n",
        "            \n",
        "        print(f\"   -> Training data: {train_img_path}\")\n",
        "        \n",
        "        train_ds = YoloDataset(train_img_path, train_lbl_path, imgsz=CFG[\"imgsz\"])\n",
        "        if len(train_ds) == 0:\n",
        "            print(\"❌ Dataset empty. Skipping.\")\n",
        "            cleanup_batch(ds)\n",
        "            continue\n",
        "        \n",
        "        # Fix: pin_memory=True for speed\n",
        "        train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
        "        \n",
        "        # 2. Train Epochs\n",
        "        # If resuming mid-batch, start_epoch applies only to the first batch\n",
        "        current_start_epoch = start_epoch if b_idx == start_batch else 0\n",
        "        \n",
        "        for epoch in range(current_start_epoch, CFG[\"epochs\"]):\n",
        "            model.train()\n",
        "            epoch_loss = 0.0\n",
        "            num_pos_sum = 0\n",
        "            \n",
        "            for i, (imgs, targets) in enumerate(train_loader):\n",
        "                imgs = imgs.to(device, non_blocking=True)\n",
        "                for k, v in targets.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        targets[k] = v.to(device, non_blocking=True)\n",
        "                \n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                \n",
        "                with torch.amp.autocast(\"cuda\", enabled=CFG[\"amp\"]):\n",
        "                    head_out = model(imgs)\n",
        "                    losses, stats = model.criterion(head_out, targets)\n",
        "                    \n",
        "                    total_loss = losses[\"loss\"]\n",
        "                \n",
        "                scaler.scale(total_loss).backward()\n",
        "                \n",
        "                if CFG.get(\"grad_clip_norm\"):\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip_norm\"])\n",
        "                    \n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                \n",
        "                # Logging\n",
        "                epoch_loss += total_loss.item()\n",
        "                num_pos_sum += stats[\"num_pos\"]\n",
        "                global_step += 1\n",
        "                \n",
        "                if i % 10 == 0:\n",
        "                    print(f\"Batch {b_idx+1} Ep {epoch+1} It {i}: Loss {total_loss.item():.4f} (Box {losses['loss_box'].item():.4f} Cls {losses['loss_cls'].item():.4f}) Pos {stats['num_pos']}\")\n",
        "                \n",
        "                # Intra-epoch Checkpoint\n",
        "                if i > 0 and i % 200 == 0:\n",
        "                    save_checkpoint(last_ckpt_path, b_idx, epoch, global_step)\n",
        "                    \n",
        "            # End of Epoch\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"✅ Epoch {epoch+1} Complete. Avg Loss: {avg_loss:.4f} Avg Pos: {num_pos_sum/len(train_loader):.1f}\")\n",
        "            \n",
        "            # Save Checkpoint\n",
        "            save_checkpoint(last_ckpt_path, b_idx, epoch + 1, global_step) # epoch+1 so we resume next\n",
        "            \n",
        "        # 3. End of Batch\n",
        "        print(f\"🎉 Batch {b_idx+1} Complete!\")\n",
        "        save_checkpoint(last_ckpt_path, b_idx, CFG[\"epochs\"], global_step, complete=True)\n",
        "        \n",
        "        # Visualization\n",
        "        visualize_batch_results(model, train_loader, device, RUN_DIR, b_idx)\n",
        "        \n",
        "        # Cleanup\n",
        "        cleanup_batch(ds)\n",
        "        \n",
        "    print(\"🏁 Training Finished!\")\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n🛑 Interrupted! Saving state...\")\n",
        "    torch.save({\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "        \"batch_idx\": b_idx,\n",
        "        \"epoch\": epoch,\n",
        "        \"global_step\": global_step,\n",
        "        \"batch_complete\": False\n",
        "    }, \"interrupted.pt\")\n",
        "    print(\"Saved interrupted.pt\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error: {e}\")\n",
        "    torch.save({\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "        \"batch_idx\": b_idx,\n",
        "        \"epoch\": epoch,\n",
        "        \"global_step\": global_step,\n",
        "        \"batch_complete\": False\n",
        "    }, \"error_state.pt\")\n",
        "    raise e\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
