{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install fiftyone\n",
        "# 1. Imports & Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import copy\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "# Set Seed\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"âœ… Notebook Updated: Version 3.0 (Fixed Collate & Criterion)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Configuration\n",
        "# --- USER SETTINGS ---\n",
        "QUICK_TEST = False  # Set to True for a fast smoke test (1 batch, 10 images)\n",
        "BATCH_SIZE = 100 if QUICK_TEST else 2000  # Images per \"roll\"\n",
        "NUM_BATCHES = 1 if QUICK_TEST else 10    # How many times to roll\n",
        "EPOCHS_PER_BATCH = 1 if QUICK_TEST else 5 # Epochs to train on each batch\n",
        "\n",
        "BASE_DIR = os.path.abspath(\"yolo-lab\")\n",
        "DIRS = {\n",
        "    \"datasets\": os.path.join(BASE_DIR, \"datasets\"),\n",
        "    \"runs\": os.path.join(BASE_DIR, \"runs\"),\n",
        "    \"configs\": os.path.join(BASE_DIR, \"configs\"),\n",
        "}\n",
        "for d in DIRS.values():\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "EXP_NAME = \"yolov8_local_batch\"\n",
        "RUN_NAME = f\"{timestamp}_{EXP_NAME}\"\n",
        "RUN_DIR = os.path.join(DIRS[\"runs\"], RUN_NAME)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "# Model & Training Config\n",
        "CFG = {\n",
        "    \"exp_name\": EXP_NAME,\n",
        "    \"run_name\": RUN_NAME,\n",
        "    \"seed\": 42,\n",
        "    \"imgsz\": 640,\n",
        "    \"batch_size\": 8 if QUICK_TEST else 16,\n",
        "    \"num_classes\": 80,\n",
        "    \n",
        "    # Model\n",
        "    \"width\": 1.0,\n",
        "    \"depth\": 1.0,\n",
        "    \"reg_max\": 16,\n",
        "    \"head_hidden\": 256,\n",
        "    \"backbone\": \"yolov8_cspdarknet\",\n",
        "    \n",
        "    # Optimizer\n",
        "    \"optimizer\": \"adamw\",\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"cosine_schedule\": True,\n",
        "    \"epochs\": EPOCHS_PER_BATCH, # Per batch\n",
        "    \"amp\": True,\n",
        "    \"grad_clip_norm\": 10.0,\n",
        "    \"ema_decay\": 0.9998,\n",
        "    \n",
        "    # Loss\n",
        "    \"tal_alpha\": 1.0,\n",
        "    \"tal_beta\": 6.0,\n",
        "    \"tal_topk\": 10,\n",
        "    \"tal_center_radius\": 2.5,\n",
        "    \"loss_weights\": {\"box\": 7.5, \"cls\": 0.5, \"dfl\": 1.5}, # Adjusted for v8\n",
        "    \n",
        "    # Augmentation\n",
        "    \"letterbox_pad\": 114,\n",
        "    \"hflip_p\": 0.5,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.7,\n",
        "    \"hsv_v\": 0.4,\n",
        "    \n",
        "    # Paths (Dynamic per batch)\n",
        "    \"data_root\": os.path.join(DIRS[\"datasets\"], \"current_batch\"),\n",
        "    \"train_img_dir\": \"images/train\",\n",
        "    \"train_lbl_dir\": \"labels/train\",\n",
        "    \"val_img_dir\": \"images/val\",\n",
        "    \"val_lbl_dir\": \"labels/val\",\n",
        "}\n",
        "\n",
        "print(\"Run Directory:\", RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Model Architecture\n",
        "def autopad(k, p=None, d=1):\n",
        "    if d > 1:\n",
        "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]\n",
        "    if p is None:\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n",
        "    return p\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    default_act = nn.SiLU()\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
        "        super().__init__()\n",
        "        c_ = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
        "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "class C2f(nn.Module):\n",
        "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
        "        super().__init__()\n",
        "        self.c = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
        "        self.cv2 = Conv((2 + n) * self.c, c2, 1)\n",
        "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3)), e=1.0) for _ in range(n))\n",
        "    def forward(self, x):\n",
        "        y = list(self.cv1(x).chunk(2, 1))\n",
        "        y.extend(m(y[-1]) for m in self.m)\n",
        "        return self.cv2(torch.cat(y, 1))\n",
        "\n",
        "class SPPF(nn.Module):\n",
        "    def __init__(self, c1, c2, k=5):\n",
        "        super().__init__()\n",
        "        c_ = c1 // 2\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n",
        "        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        y1 = self.m(x)\n",
        "        y2 = self.m(y1)\n",
        "        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n",
        "\n",
        "class CSPDarknet(nn.Module):\n",
        "    def __init__(self, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        base_c = [64, 128, 256, 512, 1024]\n",
        "        base_d = [3, 6, 6, 3]\n",
        "        self.c = [int(x * width) for x in base_c]\n",
        "        self.d = [max(round(x * depth), 1) if x > 1 else x for x in base_d]\n",
        "        self.stem = Conv(3, self.c[0], 3, 2)\n",
        "        self.stage1 = nn.Sequential(Conv(self.c[0], self.c[1], 3, 2), C2f(self.c[1], self.c[1], n=self.d[0], shortcut=True))\n",
        "        self.stage2 = nn.Sequential(Conv(self.c[1], self.c[2], 3, 2), C2f(self.c[2], self.c[2], n=self.d[1], shortcut=True))\n",
        "        self.stage3 = nn.Sequential(Conv(self.c[2], self.c[3], 3, 2), C2f(self.c[3], self.c[3], n=self.d[2], shortcut=True))\n",
        "        self.stage4 = nn.Sequential(Conv(self.c[3], self.c[4], 3, 2), C2f(self.c[4], self.c[4], n=self.d[3], shortcut=True), SPPF(self.c[4], self.c[4], k=5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        c3 = self.stage2(x)\n",
        "        c4 = self.stage3(c3)\n",
        "        c5 = self.stage4(c4)\n",
        "        return c3, c4, c5\n",
        "\n",
        "class YOLOv8PAFPN(nn.Module):\n",
        "    def __init__(self, c3, c4, c5, out_ch=256, width=1.0, depth=1.0):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce5 = Conv(c5, c4, 1, 1)\n",
        "        self.c2f_p4 = C2f(c4 + c4, c4, n=3, shortcut=False)\n",
        "        self.reduce4 = Conv(c4, c3, 1, 1)\n",
        "        self.c2f_p3 = C2f(c3 + c3, c3, n=3, shortcut=False)\n",
        "        self.down3 = Conv(c3, c3, 3, 2)\n",
        "        self.c2f_n4 = C2f(c3 + c4, c4, n=3, shortcut=False)\n",
        "        self.down4 = Conv(c4, c4, 3, 2)\n",
        "        self.c2f_n5 = C2f(c4 + c5, c5, n=3, shortcut=False)\n",
        "\n",
        "    def forward(self, c3, c4, c5):\n",
        "        p5 = c5\n",
        "        p4 = self.reduce5(p5)\n",
        "        p4_out = self.c2f_p4(torch.cat([self.up(p4), c4], dim=1))\n",
        "        p3 = self.reduce4(p4_out)\n",
        "        p3_out = self.c2f_p3(torch.cat([self.up(p3), c3], dim=1))\n",
        "        n3 = p3_out\n",
        "        n4_out = self.c2f_n4(torch.cat([self.down3(n3), p4_out], dim=1))\n",
        "        n5_out = self.c2f_n5(torch.cat([self.down4(n4_out), p5], dim=1))\n",
        "        return n3, n4_out, n5_out\n",
        "\n",
        "class Integral(nn.Module):\n",
        "    def __init__(self, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.reg_max = int(reg_max)\n",
        "        self.register_buffer(\"proj\", torch.arange(self.reg_max + 1, dtype=torch.float32), persistent=False)\n",
        "    def forward(self, logits):\n",
        "        return (logits.softmax(dim=-1) * self.proj).sum(dim=-1)\n",
        "\n",
        "class YoloV8LiteHead(nn.Module):\n",
        "    def __init__(self, in_channels_list, num_classes=80, hidden=256, reg_max=16):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.reg_max = reg_max\n",
        "        self.integral = Integral(self.reg_max)\n",
        "        self.cls_towers = nn.ModuleList()\n",
        "        self.reg_towers = nn.ModuleList()\n",
        "        self.cls_preds = nn.ModuleList()\n",
        "        self.box_preds = nn.ModuleList()\n",
        "        \n",
        "        for in_ch in in_channels_list:\n",
        "            self.cls_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.reg_towers.append(nn.Sequential(Conv(in_ch, hidden, 3, 1), Conv(hidden, hidden, 3, 1)))\n",
        "            self.cls_preds.append(nn.Conv2d(hidden, num_classes, 1))\n",
        "            self.box_preds.append(nn.Conv2d(hidden, 4 * (self.reg_max + 1), 1))\n",
        "\n",
        "    def forward(self, features):\n",
        "        cls_outs = []\n",
        "        box_outs = []\n",
        "        for i, f in enumerate(features):\n",
        "            cls_outs.append(self.cls_preds[i](self.cls_towers[i](f)))\n",
        "            box_outs.append(self.box_preds[i](self.reg_towers[i](f)))\n",
        "        return cls_outs, box_outs\n",
        "\n",
        "class YoloModel(nn.Module):\n",
        "    def __init__(self, num_classes=80, backbone=\"yolov8_cspdarknet\", head_hidden=256, fpn_out=256):\n",
        "        super().__init__()\n",
        "        width = CFG.get(\"width\", 1.0)\n",
        "        depth = CFG.get(\"depth\", 1.0)\n",
        "        self.backbone = CSPDarknet(width=width, depth=depth)\n",
        "        base_c = [256, 512, 1024]\n",
        "        c3, c4, c5 = [int(x * width) for x in base_c]\n",
        "        self.neck = YOLOv8PAFPN(c3=c3, c4=c4, c5=c5, out_ch=fpn_out, width=width, depth=depth)\n",
        "        self.head = YoloV8LiteHead(in_channels_list=[c3, c4, c5], num_classes=num_classes, hidden=head_hidden, reg_max=CFG.get(\"reg_max\", 16))\n",
        "        self.strides = [8, 16, 32]\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        c3, c4, c5 = self.backbone(x)\n",
        "        p3, p4, p5 = self.neck(c3, c4, c5)\n",
        "        cls_outs, box_outs = self.head([p3, p4, p5])\n",
        "        head_out = {\"features\": [p3, p4, p5], \"cls\": cls_outs, \"box\": box_outs, \"strides\": self.strides}\n",
        "        \n",
        "        if self.training and targets is not None and hasattr(self, \"criterion\"):\n",
        "            losses, stats = self.criterion(head_out, targets)\n",
        "            return losses, stats\n",
        "        return head_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Utils & Loss\n",
        "def make_grid(h, w, stride, device):\n",
        "    ys = torch.arange(h, device=device)\n",
        "    xs = torch.arange(w, device=device)\n",
        "    yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
        "    cx = (xx + 0.5) * stride\n",
        "    cy = (yy + 0.5) * stride\n",
        "    return cx.reshape(-1), cy.reshape(-1)\n",
        "\n",
        "def box_iou_xyxy_matrix(a, b):\n",
        "    if a.numel() == 0 or b.numel() == 0: return a.new_zeros((a.shape[0], b.shape[0]))\n",
        "    area_a = ((a[:, 2] - a[:, 0]).clamp(min=0) * (a[:, 3] - a[:, 1]).clamp(min=0))[:, None]\n",
        "    area_b = ((b[:, 2] - b[:, 0]).clamp(min=0) * (b[:, 3] - b[:, 1]).clamp(min=0))[None, :]\n",
        "    x1 = torch.maximum(a[:, None, 0], b[None, :, 0])\n",
        "    y1 = torch.maximum(a[:, None, 1], b[None, :, 1])\n",
        "    x2 = torch.minimum(a[:, None, 2], b[None, :, 2])\n",
        "    y2 = torch.minimum(a[:, None, 3], b[None, :, 3])\n",
        "    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n",
        "    return inter / (area_a + area_b - inter + 1e-6)\n",
        "\n",
        "def bbox_iou(box1, box2, eps=1e-7):\n",
        "    # box1: [N, 4], box2: [N, 4]\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
        "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n",
        "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n",
        "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
        "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
        "    union = w1 * h1 + w2 * h2 - inter + eps\n",
        "    return inter / union\n",
        "\n",
        "class DetectionLoss(nn.Module):\n",
        "    def __init__(self, num_classes, image_size, strides, lambda_box=7.5, lambda_cls=0.5, dfl_ch=17):\n",
        "        super().__init__()\n",
        "        self.nc = num_classes\n",
        "        self.imgsz = image_size\n",
        "        self.strides = strides\n",
        "        self.lambda_box = lambda_box\n",
        "        self.lambda_cls = lambda_cls\n",
        "        self.dfl_ch = dfl_ch\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, head_out, targets):\n",
        "        cls_outs = head_out[\"cls\"]\n",
        "        box_outs = head_out[\"box\"]\n",
        "        \n",
        "        targets_per_image, levels = self.build_targets(cls_outs, box_outs, targets)\n",
        "        \n",
        "        loss_cls = torch.tensor(0.0, device=cls_outs[0].device)\n",
        "        loss_box = torch.tensor(0.0, device=cls_outs[0].device)\n",
        "        num_pos_total = 0.0\n",
        "        \n",
        "        for b in range(len(targets_per_image)):\n",
        "            t = targets_per_image[b]\n",
        "            pos_mask = t[\"pos_index\"]\n",
        "            num_pos = len(pos_mask)\n",
        "            num_pos_total += num_pos\n",
        "            \n",
        "            # Classification Loss\n",
        "            pred_cls = torch.cat([c[b].permute(1,2,0).reshape(-1, self.nc) for c in cls_outs], 0)\n",
        "            t_cls = torch.zeros_like(pred_cls)\n",
        "            if num_pos > 0:\n",
        "                t_cls[pos_mask] = t[\"t_cls_soft\"].to(t_cls.dtype)\n",
        "            \n",
        "            l_cls = self.bce(pred_cls, t_cls).sum()\n",
        "            loss_cls += l_cls\n",
        "            \n",
        "            # Box Loss (IoU + DFL)\n",
        "            if num_pos > 0:\n",
        "                pred_box_dist = torch.cat([x[b].permute(1,2,0).reshape(-1, 4 * self.dfl_ch) for x in box_outs], 0)\n",
        "                pred_box_pos = pred_box_dist[pos_mask].view(-1, 4, self.dfl_ch)\n",
        "                \n",
        "                # DFL\n",
        "                t_box_ltrb = t[\"t_box_ltrb\"]\n",
        "                tl = t_box_ltrb.long()\n",
        "                tr = tl + 1\n",
        "                wl = tr.float() - t_box_ltrb\n",
        "                wr = t_box_ltrb - tl.float()\n",
        "                \n",
        "                l_dfl = (F.cross_entropy(pred_box_pos.view(-1, self.dfl_ch), tl.view(-1), reduction=\"none\").view(-1, 4) * wl +\n",
        "                         F.cross_entropy(pred_box_pos.view(-1, self.dfl_ch), tr.view(-1), reduction=\"none\").view(-1, 4) * wr).mean(-1)\n",
        "                \n",
        "                # IoU\n",
        "                # Decode box for IoU\n",
        "                anchors_cx, anchors_cy = [], []\n",
        "                for level_i, level in enumerate(levels):\n",
        "                    cx, cy = make_grid(level[\"H\"], level[\"W\"], level[\"stride\"], pred_box_dist.device)\n",
        "                    anchors_cx.append(cx)\n",
        "                    anchors_cy.append(cy)\n",
        "                anchors_cx = torch.cat(anchors_cx)[pos_mask]\n",
        "                anchors_cy = torch.cat(anchors_cy)[pos_mask]\n",
        "                \n",
        "                pred_ltrb = (pred_box_pos.softmax(dim=-1) * torch.arange(self.dfl_ch, device=pred_box_pos.device).float()).sum(dim=-1)\n",
        "                \n",
        "                pred_x1 = anchors_cx - pred_ltrb[:, 0]\n",
        "                pred_y1 = anchors_cy - pred_ltrb[:, 1]\n",
        "                pred_x2 = anchors_cx + pred_ltrb[:, 2]\n",
        "                pred_y2 = anchors_cy + pred_ltrb[:, 3]\n",
        "                pred_xyxy = torch.stack([pred_x1, pred_y1, pred_x2, pred_y2], dim=-1)\n",
        "                \n",
        "                iou = bbox_iou(pred_xyxy, t[\"t_box_xyxy\"])\n",
        "                l_iou = 1.0 - iou\n",
        "                \n",
        "                loss_box += (l_iou + l_dfl).sum() * self.lambda_box\n",
        "\n",
        "        # Normalize\n",
        "        norm = max(num_pos_total, 1.0)\n",
        "        loss_cls = (loss_cls * self.lambda_cls) / norm\n",
        "        loss_box = loss_box / norm\n",
        "        \n",
        "        return {\"loss\": loss_cls + loss_box, \"loss_cls\": loss_cls, \"loss_box\": loss_box}, {\"num_pos\": num_pos_total}\n",
        "\n",
        "    def build_targets(self, cls_outs, box_outs, targets):\n",
        "        gt_classes = targets[\"labels\"]\n",
        "        gt_boxes = targets[\"boxes\"]\n",
        "        batch_idx = targets[\"batch_index\"]\n",
        "        \n",
        "        B = cls_outs[0].shape[0]\n",
        "        gt_cls_list = []\n",
        "        gt_box_list = []\n",
        "        for i in range(B):\n",
        "            mask = batch_idx == i\n",
        "            gt_cls_list.append(gt_classes[mask])\n",
        "            gt_box_list.append(gt_boxes[mask])\n",
        "            \n",
        "        return build_targets_task_aligned(cls_outs, box_outs, self.strides, gt_cls_list, gt_box_list, self.imgsz)\n",
        "\n",
        "def build_targets_task_aligned(cls_outs, box_outs, strides, gt_classes, gt_boxes_xyxy, image_size):\n",
        "    device = cls_outs[0].device\n",
        "    B = cls_outs[0].shape[0]\n",
        "    C = cls_outs[0].shape[1]\n",
        "    \n",
        "    levels = []\n",
        "    start = 0\n",
        "    grids = []\n",
        "    \n",
        "    for (cl, s) in zip(cls_outs, strides):\n",
        "        _, _, H, W = cl.shape\n",
        "        levels.append({\"H\": H, \"W\": W, \"stride\": s, \"start\": start, \"end\": start + H * W})\n",
        "        cx, cy = make_grid(H, W, s, device)\n",
        "        grids.append((cx, cy))\n",
        "        start += H * W\n",
        "        \n",
        "    tal_alpha = float(CFG.get(\"tal_alpha\", 1.0))\n",
        "    tal_beta = float(CFG.get(\"tal_beta\", 6.0))\n",
        "    tal_topk = int(CFG.get(\"tal_topk\", 10))\n",
        "    tal_cr = float(CFG.get(\"tal_center_radius\", 2.5))\n",
        "    \n",
        "    per_image_targets = []\n",
        "    for b in range(B):\n",
        "        cls_per_image = [cl[b].permute(1, 2, 0).reshape(-1, C) for cl in cls_outs]\n",
        "        cls_flat = torch.cat(cls_per_image, dim=0)\n",
        "        N_total = cls_flat.shape[0]\n",
        "        \n",
        "        gtc = gt_classes[b]\n",
        "        gtb = gt_boxes_xyxy[b]\n",
        "        Ng = int(gtc.numel())\n",
        "        \n",
        "        if Ng == 0:\n",
        "            per_image_targets.append({\n",
        "                \"t_cls_soft\": torch.zeros(0, C, device=device),\n",
        "                \"t_box_xyxy\": torch.zeros(0, 4, device=device),\n",
        "                \"t_box_ltrb\": torch.zeros(0, 4, device=device),\n",
        "                \"pos_index\": torch.zeros(0, dtype=torch.long, device=device),\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        pred_xyxy_levels = []\n",
        "        for (bx, level, (cx, cy)) in zip(box_outs, levels, grids):\n",
        "            H, W, s = level[\"H\"], level[\"W\"], level[\"stride\"]\n",
        "            bl = bx[b]\n",
        "            M1 = bl.shape[0] // 4\n",
        "            bl = bl.view(4, M1, H, W).permute(2, 3, 0, 1).reshape(H * W, 4, M1)\n",
        "            probs = bl.softmax(dim=-1)\n",
        "            proj = torch.arange(M1, device=device, dtype=bl.dtype)\n",
        "            dists = (probs * proj).sum(dim=-1) * float(s)\n",
        "            \n",
        "            x1 = cx - dists[:, 0]\n",
        "            y1 = cy - dists[:, 1]\n",
        "            x2 = cx + dists[:, 2]\n",
        "            y2 = cy + dists[:, 3]\n",
        "            pred_xyxy_levels.append(torch.stack([x1, y1, x2, y2], dim=-1).clamp_(0, image_size))\n",
        "            \n",
        "        pred_xyxy = torch.cat(pred_xyxy_levels, dim=0)\n",
        "\n",
        "        candidate_mask = torch.zeros(N_total, Ng, dtype=torch.bool, device=device)\n",
        "        for level, (cx, cy) in enumerate(grids):\n",
        "            start, end, s = levels[level][\"start\"], levels[level][\"end\"], levels[level][\"stride\"]\n",
        "            Nl = end - start\n",
        "            cxv, cyv = cx.view(Nl, 1), cy.view(Nl, 1)\n",
        "            \n",
        "            if tal_cr > 0:\n",
        "                gt_centers = 0.5 * (gtb[:, :2] + gtb[:, 2:])\n",
        "                half = tal_cr * s\n",
        "                in_center = (cxv >= gt_centers[:, 0] - half) & (cyv >= gt_centers[:, 1] - half) & \\\n",
        "                            (cxv <= gt_centers[:, 0] + half) & (cyv <= gt_centers[:, 1] + half)\n",
        "                candidate_mask[start:end] |= in_center\n",
        "            else:\n",
        "                in_box = (cxv >= gtb[:, 0]) & (cyv >= gtb[:, 1]) & (cxv <= gtb[:, 2]) & (cyv <= gtb[:, 3])\n",
        "                candidate_mask[start:end] |= in_box\n",
        "                \n",
        "        cls_sigmoid = cls_flat.sigmoid()\n",
        "        cls_gt_scores = cls_sigmoid[:, gtc]\n",
        "        iou_matrix = box_iou_xyxy_matrix(pred_xyxy, gtb)\n",
        "        align = (cls_gt_scores.clamp(min=1e-9).pow(tal_alpha)) * (iou_matrix.clamp(min=1e-9).pow(tal_beta))\n",
        "        align = torch.where(candidate_mask, align, torch.full_like(align, -1e-9))\n",
        "        \n",
        "        k = min(tal_topk, align.shape[0])\n",
        "        topk_scores, topk_index = torch.topk(align, k, dim=0)\n",
        "        \n",
        "        best_gt_per_pred = torch.full((N_total,), -1, dtype=torch.long, device=device)\n",
        "        best_score_per_pred = torch.full((N_total,), -1e-9, dtype=align.dtype, device=device)\n",
        "        \n",
        "        for j in range(Ng):\n",
        "            idx_j = topk_index[:, j]\n",
        "            score_j = topk_scores[:, j]\n",
        "            better = score_j > best_score_per_pred[idx_j]\n",
        "            best_gt_per_pred[idx_j[better]] = j\n",
        "            best_score_per_pred[idx_j[better]] = score_j[better]\n",
        "            \n",
        "        pos_mask = best_gt_per_pred >= 0\n",
        "        pos_index = torch.nonzero(pos_mask, as_tuple=False).squeeze(1)\n",
        "        \n",
        "        if pos_index.numel() == 0:\n",
        "            per_image_targets.append({\n",
        "                \"t_cls_soft\": torch.zeros(0, C, device=device),\n",
        "                \"t_box_xyxy\": torch.zeros(0, 4, device=device),\n",
        "                \"t_box_ltrb\": torch.zeros(0, 4, device=device),\n",
        "                \"pos_index\": pos_index,\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        gt_index = best_gt_per_pred[pos_index]\n",
        "        scores = best_score_per_pred[pos_index].clamp(min=0.0)\n",
        "        t_cls_soft = torch.zeros(len(pos_index), C, device=device)\n",
        "        t_cls_soft[torch.arange(len(pos_index)), gtc[gt_index]] = scores\n",
        "        t_box_xyxy = gtb[gt_index]\n",
        "        \n",
        "        t_box_ltrb = torch.empty(len(pos_index), 4, device=device)\n",
        "        for level_i, level in enumerate(levels):\n",
        "            start, end, s = level[\"start\"], level[\"end\"], level[\"stride\"]\n",
        "            cx, cy = grids[level_i]\n",
        "            in_level = (pos_index >= start) & (pos_index < end)\n",
        "            if in_level.any():\n",
        "                idx_l = pos_index[in_level] - start\n",
        "                ct = torch.stack((cx[idx_l], cy[idx_l]), dim=-1)\n",
        "                gs = gtb[gt_index[in_level]]\n",
        "                t_box_ltrb[in_level] = torch.stack((ct[:,0]-gs[:,0], ct[:,1]-gs[:,1], gs[:,2]-ct[:,0], gs[:,3]-ct[:,1]), dim=-1).clamp(min=0, max=float(image_size))\n",
        "\n",
        "        per_image_targets.append({\n",
        "            \"t_cls_soft\": t_cls_soft,\n",
        "            \"t_box_xyxy\": t_box_xyxy,\n",
        "            \"t_box_ltrb\": t_box_ltrb,\n",
        "            \"pos_index\": pos_index,\n",
        "        })\n",
        "    \n",
        "    return per_image_targets, levels\n",
        "\n",
        "# --- Visualization Utils ---\n",
        "def decode_outputs(head_out, strides, conf_thres=0.25, iou_thres=0.45, max_det=300):\n",
        "    # head_out: dict with 'cls' and 'box'\n",
        "    device = head_out[\"cls\"][0].device\n",
        "    cls_outs = head_out[\"cls\"]\n",
        "    box_outs = head_out[\"box\"]\n",
        "    B = cls_outs[0].shape[0]\n",
        "    \n",
        "    preds = []\n",
        "    \n",
        "    # Generate grids\n",
        "    grids = []\n",
        "    for (cl, s) in zip(cls_outs, strides):\n",
        "        _, _, H, W = cl.shape\n",
        "        cx, cy = make_grid(H, W, s, device)\n",
        "        grids.append((cx, cy, s))\n",
        "        \n",
        "    for b in range(B):\n",
        "        # Collect all predictions for this image\n",
        "        batch_boxes = []\n",
        "        batch_scores = []\n",
        "        batch_classes = []\n",
        "        \n",
        "        for i, (bx, cx, cy, s) in enumerate(zip(box_outs, *zip(*grids))):\n",
        "            # Box decoding\n",
        "            bl = bx[b] # [4*reg_max, H, W]\n",
        "            C_box = bl.shape[0]\n",
        "            reg_max = C_box // 4\n",
        "            H, W = bl.shape[1], bl.shape[2]\n",
        "            \n",
        "            bl = bl.view(4, reg_max, H*W).permute(2, 0, 1) # [HW, 4, reg_max]\n",
        "            probs = bl.softmax(dim=-1)\n",
        "            proj = torch.arange(reg_max, device=device, dtype=bl.dtype)\n",
        "            ltrb = (probs * proj).sum(dim=-1) * s # [HW, 4]\n",
        "            \n",
        "            x1 = cx - ltrb[:, 0]\n",
        "            y1 = cy - ltrb[:, 1]\n",
        "            x2 = cx + ltrb[:, 2]\n",
        "            y2 = cy + ltrb[:, 3]\n",
        "            xyxy = torch.stack([x1, y1, x2, y2], dim=-1)\n",
        "            \n",
        "            # Class scores\n",
        "            cl = cls_outs[i][b].permute(1, 2, 0).reshape(-1, cls_outs[i].shape[1]) # [HW, nc]\n",
        "            scores = cl.sigmoid()\n",
        "            \n",
        "            # Filter by conf\n",
        "            max_scores, max_classes = scores.max(dim=1)\n",
        "            mask = max_scores > conf_thres\n",
        "            \n",
        "            if mask.any():\n",
        "                batch_boxes.append(xyxy[mask])\n",
        "                batch_scores.append(max_scores[mask])\n",
        "                batch_classes.append(max_classes[mask])\n",
        "                \n",
        "        if not batch_boxes:\n",
        "            preds.append(torch.zeros(0, 6, device=device))\n",
        "            continue\n",
        "            \n",
        "        batch_boxes = torch.cat(batch_boxes, dim=0)\n",
        "        batch_scores = torch.cat(batch_scores, dim=0)\n",
        "        batch_classes = torch.cat(batch_classes, dim=0)\n",
        "        \n",
        "        # NMS\n",
        "        keep = torch.ops.torchvision.nms(batch_boxes, batch_scores, iou_thres)\n",
        "        keep = keep[:max_det]\n",
        "        \n",
        "        # [x1, y1, x2, y2, conf, cls]\n",
        "        out = torch.cat([batch_boxes[keep], batch_scores[keep, None], batch_classes[keep, None].float()], dim=1)\n",
        "        preds.append(out)\n",
        "        \n",
        "    return preds\n",
        "\n",
        "def visualize_batch_results(model, loader, device, run_dir, batch_idx, num_samples=3):\n",
        "    model.eval()\n",
        "    images, targets = next(iter(loader))\n",
        "    images = images.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        head_out = model(images)\n",
        "        preds = decode_outputs(head_out, model.strides)\n",
        "        \n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    if num_samples == 1: axes = [axes]\n",
        "    \n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        img = np.ascontiguousarray(img) * 255\n",
        "        img = img.astype(np.uint8)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        \n",
        "        # GT\n",
        "        gt_mask = targets[\"batch_index\"] == i\n",
        "        gt_boxes = targets[\"boxes\"][gt_mask]\n",
        "        for box in gt_boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.tolist())\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2) # Green\n",
        "            \n",
        "        # Pred\n",
        "        pred = preds[i]\n",
        "        for p in pred:\n",
        "            x1, y1, x2, y2, conf, cls = p.tolist()\n",
        "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2) # Red\n",
        "            cv2.putText(img, f\"{conf:.2f}\", (int(x1), int(y1)-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "            \n",
        "        if num_samples > 1:\n",
        "            axes[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "            axes[i].axis(\"off\")\n",
        "            axes[i].set_title(f\"Sample {i}\")\n",
        "        else:\n",
        "            axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "            axes[0].axis(\"off\")\n",
        "            axes[0].set_title(f\"Sample {i}\")\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(run_dir, f\"val_batch_{batch_idx}.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    print(f\"ðŸ–¼ï¸ Visualization saved to {save_path}\")\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Dataset & Dataloader\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, imgsz=640, augment=True, pad_value=114):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.imgsz = imgsz\n",
        "        self.augment = augment\n",
        "        self.pad_value = pad_value\n",
        "        \n",
        "        # Support multiple extensions\n",
        "        self.image_paths = []\n",
        "        for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"):\n",
        "            self.image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        \n",
        "        self.label_paths = [os.path.join(label_dir, Path(p).stem + \".txt\") for p in self.image_paths]\n",
        "        \n",
        "        if len(self.image_paths) == 0:\n",
        "            print(f\"âš ï¸ WARNING: No images found in {image_dir}\")\n",
        "            print(f\"   Did the export work? Check {os.path.dirname(image_dir)}\")\n",
        "        else:\n",
        "            print(f\"âœ… Loaded {len(self.image_paths)} images from {image_dir}\")\n",
        "        \n",
        "    def __len__(self): return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = cv2.imread(self.image_paths[index])\n",
        "        h, w = img.shape[:2]\n",
        "        \n",
        "        # Read labels\n",
        "        lbl_path = self.label_paths[index]\n",
        "        boxes = []\n",
        "        cls = []\n",
        "        if os.path.exists(lbl_path):\n",
        "            with open(lbl_path) as f:\n",
        "                for line in f:\n",
        "                    parts = list(map(float, line.strip().split()))\n",
        "                    if len(parts) == 5:\n",
        "                        cls.append(int(parts[0]))\n",
        "                        # YOLO xywh to xyxy\n",
        "                        cx, cy, bw, bh = parts[1:]\n",
        "                        x1 = (cx - bw/2) * w\n",
        "                        y1 = (cy - bh/2) * h\n",
        "                        x2 = (cx + bw/2) * w\n",
        "                        y2 = (cy + bh/2) * h\n",
        "                        boxes.append([x1, y1, x2, y2])\n",
        "        \n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
        "        cls = torch.tensor(cls, dtype=torch.long) if cls else torch.zeros((0,), dtype=torch.long)\n",
        "        \n",
        "        # Letterbox (Simplified)\n",
        "        r = min(self.imgsz / h, self.imgsz / w)\n",
        "        nw, nh = int(w * r), int(h * r)\n",
        "        img = cv2.resize(img, (nw, nh))\n",
        "        \n",
        "        # Pad\n",
        "        pad_w = self.imgsz - nw\n",
        "        pad_h = self.imgsz - nh\n",
        "        img = cv2.copyMakeBorder(img, pad_h//2, pad_h-pad_h//2, pad_w//2, pad_w-pad_w//2, cv2.BORDER_CONSTANT, value=(114,114,114))\n",
        "        \n",
        "        # Adjust boxes\n",
        "        if len(boxes):\n",
        "            boxes[:, [0, 2]] = boxes[:, [0, 2]] * r + pad_w//2\n",
        "            boxes[:, [1, 3]] = boxes[:, [1, 3]] * r + pad_h//2\n",
        "            \n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
        "        \n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": cls,\n",
        "            \"image_id\": Path(self.image_paths[index]).stem,\n",
        "            \"orig_size\": (h, w),\n",
        "            \"scale\": r,\n",
        "            \"pad\": (pad_w//2, pad_h//2)\n",
        "        }\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = list(zip(*batch))\n",
        "    images = torch.stack(images, dim=0)\n",
        "    \n",
        "    all_boxes = []\n",
        "    all_labels = []\n",
        "    all_bidx = []\n",
        "    image_ids = []\n",
        "    scales = []\n",
        "    pads = []\n",
        "    orig_sizes = []\n",
        "    \n",
        "    for i, t in enumerate(targets):\n",
        "        n = t[\"boxes\"].shape[0]\n",
        "        if n:\n",
        "            all_boxes.append(t[\"boxes\"])\n",
        "            all_labels.append(t[\"labels\"])\n",
        "            all_bidx.append(torch.full((n,), i, dtype=torch.long))\n",
        "            \n",
        "        image_ids.append(t[\"image_id\"])\n",
        "        scales.append(t[\"scale\"])\n",
        "        pads.append(t[\"pad\"])\n",
        "        orig_sizes.append(t[\"orig_size\"])\n",
        "        \n",
        "    if len(all_boxes):\n",
        "        boxes = torch.cat(all_boxes, 0)\n",
        "        labels = torch.cat(all_labels, 0)\n",
        "        bidx = torch.cat(all_bidx, 0)\n",
        "    else:\n",
        "        boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "        labels = torch.zeros((0,), dtype=torch.long)\n",
        "        bidx = torch.zeros((0,), dtype=torch.long)\n",
        "        \n",
        "    return images, {\n",
        "        \"boxes\": boxes,\n",
        "        \"labels\": labels,\n",
        "        \"batch_index\": bidx,\n",
        "        \"image_id\": image_ids,\n",
        "        \"scale\": scales,\n",
        "        \"pad\": pads,\n",
        "        \"orig_size\": orig_sizes\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Batch Management Logic\n",
        "def prepare_batch(batch_idx, size=2000):\n",
        "    print(f\"\\nðŸ“¦ Preparing Batch {batch_idx} (Size: {size})...\")\n",
        "    \n",
        "    current_batch_name = f\"batch_{batch_idx}\"\n",
        "    \n",
        "    # 1. Safe Cleanup of Old Batches\n",
        "    if foz.list_datasets():\n",
        "        for name in foz.list_datasets():\n",
        "            if name.startswith(\"batch_\") and name != current_batch_name:\n",
        "                print(f\"ðŸ§¹ Deleting old dataset: {name}\")\n",
        "                fo.delete_dataset(name)\n",
        "    \n",
        "    # Force delete current if exists to ensure fresh download/export\n",
        "    if current_batch_name in foz.list_datasets():\n",
        "         print(f\"â™»ï¸  Deleting existing {current_batch_name} to ensure fresh start\")\n",
        "         fo.delete_dataset(current_batch_name)\n",
        "\n",
        "    # 2. Download/Load from Zoo\n",
        "    dataset = foz.load_zoo_dataset(\n",
        "        \"coco-2017\",\n",
        "        split=\"train\", \n",
        "        label_types=[\"detections\"],\n",
        "        max_samples=size,\n",
        "        shuffle=True,\n",
        "        seed=batch_idx * 999, \n",
        "        dataset_name=current_batch_name,\n",
        "        drop_existing=True \n",
        "    )\n",
        "    \n",
        "    # 3. Export to YOLO format\n",
        "    out_dir = CFG[\"data_root\"]\n",
        "    if os.path.exists(out_dir):\n",
        "        shutil.rmtree(out_dir) \n",
        "        \n",
        "    dataset.export(\n",
        "        export_dir=out_dir,\n",
        "        dataset_type=fo.types.YOLOv5Dataset,\n",
        "        label_field=\"ground_truth\",\n",
        "    )\n",
        "    print(f\"âœ… Batch {batch_idx} exported to {out_dir}\")\n",
        "    return dataset\n",
        "\n",
        "def cleanup_batch(dataset):\n",
        "    print(\"ðŸ§¹ Cleaning up batch...\")\n",
        "    dataset.delete()\n",
        "    if os.path.exists(CFG[\"data_root\"]):\n",
        "        shutil.rmtree(CFG[\"data_root\"])\n",
        "    print(\"âœ¨ Cleanup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Batch 4 Ep 1 It 80: Loss 0.0000 (Box 0.0000 Cls 0.0000)\n"
          ]
        }
      ],
      "source": [
        "# 7. Main Execution Loop\n",
        "state_file = os.path.join(DIRS[\"runs\"], \"checkpoint_state.json\")\n",
        "last_ckpt_path = os.path.join(RUN_DIR, \"last.pt\")\n",
        "\n",
        "# --- Initialization (ONCE) ---\n",
        "model = YoloModel(num_classes=CFG[\"num_classes\"]).to(device)\n",
        "criterion = DetectionLoss(\n",
        "    num_classes=CFG[\"num_classes\"],\n",
        "    image_size=CFG[\"imgsz\"],\n",
        "    strides=[8, 16, 32],\n",
        "    lambda_box=CFG[\"loss_weights\"][\"box\"],\n",
        "    lambda_cls=CFG[\"loss_weights\"][\"cls\"]\n",
        ")\n",
        "model.criterion = criterion\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"])\n",
        "scaler = GradScaler(enabled=CFG[\"amp\"])\n",
        "\n",
        "# --- Resume Logic ---\n",
        "start_batch = 0\n",
        "start_epoch = 0\n",
        "global_step = 0\n",
        "\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    print(f\"ðŸ”„ Resuming from {last_ckpt_path}\")\n",
        "    ckpt = torch.load(last_ckpt_path)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "    \n",
        "    # Restore state\n",
        "    start_batch = ckpt.get(\"batch_idx\", 0)\n",
        "    start_epoch = ckpt.get(\"epoch\", 0)\n",
        "    global_step = ckpt.get(\"global_step\", 0)\n",
        "    \n",
        "    # If we finished the last batch completely, move to next\n",
        "    if ckpt.get(\"batch_complete\", False):\n",
        "        start_batch += 1\n",
        "        start_epoch = 0\n",
        "        \n",
        "    print(f\"   -> Batch: {start_batch}, Epoch: {start_epoch}, Step: {global_step}\")\n",
        "\n",
        "# --- Helper: Save Checkpoint ---\n",
        "def save_checkpoint(path, batch_idx, epoch, step, complete=False):\n",
        "    torch.save({\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict(),\n",
        "        \"batch_idx\": batch_idx,\n",
        "        \"epoch\": epoch,\n",
        "        \"global_step\": step,\n",
        "        \"batch_complete\": complete\n",
        "    }, path)\n",
        "    # Also save state json for easy reading\n",
        "    with open(state_file, \"w\") as f:\n",
        "        json.dump({\"batch_idx\": batch_idx, \"epoch\": epoch, \"global_step\": step}, f)\n",
        "\n",
        "# --- Main Loop ---\n",
        "try:\n",
        "    for b_idx in range(start_batch, NUM_BATCHES):\n",
        "        print(f\"\\n=== STARTING BATCH {b_idx + 1}/{NUM_BATCHES} === \")\n",
        "        \n",
        "        # 1. Prepare Data\n",
        "        ds = prepare_batch(b_idx, size=BATCH_SIZE)\n",
        "        \n",
        "        # Detect images folder\n",
        "        if os.path.isdir(os.path.join(CFG[\"data_root\"], \"images/train\")):\n",
        "            train_img_path = os.path.join(CFG[\"data_root\"], \"images/train\")\n",
        "            train_lbl_path = os.path.join(CFG[\"data_root\"], \"labels/train\")\n",
        "        elif os.path.isdir(os.path.join(CFG[\"data_root\"], \"images/val\")):\n",
        "            print(\"âš ï¸ Using images/val as training source\")\n",
        "            train_img_path = os.path.join(CFG[\"data_root\"], \"images/val\")\n",
        "            train_lbl_path = os.path.join(CFG[\"data_root\"], \"labels/val\")\n",
        "        else:\n",
        "            # Maybe root?\n",
        "            train_img_path = CFG[\"data_root\"]\n",
        "            train_lbl_path = CFG[\"data_root\"] \n",
        "            \n",
        "        print(f\"   -> Training data: {train_img_path}\")\n",
        "        \n",
        "        train_ds = YoloDataset(train_img_path, train_lbl_path, imgsz=CFG[\"imgsz\"])\n",
        "        if len(train_ds) == 0:\n",
        "            print(\"âŒ Dataset empty. Skipping.\")\n",
        "            cleanup_batch(ds)\n",
        "            continue\n",
        "            \n",
        "        train_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
        "        \n",
        "        # 2. Train Epochs\n",
        "        # If resuming mid-batch, start_epoch applies only to the first batch\n",
        "        current_start_epoch = start_epoch if b_idx == start_batch else 0\n",
        "        \n",
        "        for epoch in range(current_start_epoch, CFG[\"epochs\"]):\n",
        "            model.train()\n",
        "            epoch_loss = 0.0\n",
        "            num_pos_sum = 0\n",
        "            \n",
        "            for i, (imgs, targets) in enumerate(train_loader):\n",
        "                imgs = imgs.to(device)\n",
        "                for k, v in targets.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        targets[k] = v.to(device)\n",
        "                \n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                \n",
        "                with torch.amp.autocast(\"cuda\", enabled=CFG[\"amp\"]):\n",
        "                    head_out = model(imgs)\n",
        "                    losses, stats = model.criterion(head_out, targets)\n",
        "                    \n",
        "                    total_loss = losses[\"loss\"]\n",
        "                \n",
        "                scaler.scale(total_loss).backward()\n",
        "                \n",
        "                if CFG.get(\"grad_clip_norm\"):\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip_norm\"])\n",
        "                    \n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                \n",
        "                # Logging\n",
        "                epoch_loss += total_loss.item()\n",
        "                num_pos_sum += stats[\"num_pos\"]\n",
        "                global_step += 1\n",
        "                \n",
        "                if i % 10 == 0:\n",
        "                    print(f\"Batch {b_idx+1} Ep {epoch+1} It {i}: Loss {total_loss.item():.4f} (Box {losses['loss_box']:.4f} Cls {losses['loss_cls']:.4f}) Pos {stats['num_pos']}\")\n",
        "                \n",
        "                # Intra-epoch Checkpoint\n",
        "                if i > 0 and i % 200 == 0:\n",
        "                    save_checkpoint(last_ckpt_path, b_idx, epoch, global_step)\n",
        "                    \n",
        "            # End of Epoch\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"âœ… Epoch {epoch+1} Complete. Avg Loss: {avg_loss:.4f} Avg Pos: {num_pos_sum/len(train_loader):.1f}\")\n",
        "            \n",
        "            # Save Checkpoint\n",
        "            save_checkpoint(last_ckpt_path, b_idx, epoch + 1, global_step) # epoch+1 so we resume next\n",
        "            \n",
        "        # 3. End of Batch\n",
        "        print(f\"ðŸŽ‰ Batch {b_idx+1} Complete!\")\n",
        "        save_checkpoint(last_ckpt_path, b_idx, CFG[\"epochs\"], global_step, complete=True)\n",
        "        \n",
        "        # Visualization\n",
        "        visualize_batch_results(model, train_loader, device, RUN_DIR, b_idx)\n",
        "        \n",
        "        # Cleanup\n",
        "        cleanup_batch(ds)\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nðŸ›‘ Interrupted! Saving state...\")\n",
        "    save_checkpoint(os.path.join(RUN_DIR, \"interrupted.pt\"), b_idx, epoch, global_step)\n",
        "    print(\"Saved interrupted.pt\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error: {e}\")\n",
        "    save_checkpoint(os.path.join(RUN_DIR, \"error_state.pt\"), b_idx, epoch, global_step)\n",
        "    raise e\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
